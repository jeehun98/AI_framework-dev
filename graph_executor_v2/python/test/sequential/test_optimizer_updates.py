# File: test_optimizer_updates.py
from __future__ import annotations
import os, sys, math, traceback

# --- Path setup (adjust if your repo layout differs) ---
THIS = os.path.abspath(os.path.dirname(__file__))
ROOT = os.path.abspath(os.path.join(THIS, "..", ".."))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

import cupy as cp

from graph_executor_v2.layers.sequential import Sequential
from graph_executor_v2.layers.dense_gemm import Dense
from graph_executor_v2.losses.softmax_ce import SoftmaxCrossEntropy

# Try SGDOpt, fallback to AdamWOpt
try:
    from graph_executor_v2.optim.sgd import SGDOpt  # type: ignore
    OPT_CLS = SGDOpt
    OPT_KW = dict(lr=0.1)
    OPT_NAME = "SGDOpt"
except Exception:
    from graph_executor_v2.optim.adamw import AdamWOpt  # type: ignore
    OPT_CLS = AdamWOpt
    OPT_KW = dict(lr=1e-1)
    OPT_NAME = "AdamWOpt"

def _param_obj(item):
    """(p,g,tag)/(p,g)/p â†’ p"""
    if isinstance(item, tuple):
        return item[0]
    return item

def _grad_obj(item):
    """(p,g,tag)/(p,g)/p â†’ g or p.grad/None"""
    if isinstance(item, tuple):
        if len(item) >= 2:
            return item[1]
        item = item[0]
    return getattr(item, "grad", None)

def _tag(item, default="(no-tag)"):
    if isinstance(item, tuple):
        if len(item) == 3:
            return str(item[2])
        return default
    return default

def _arr(x):
    p = _param_obj(x)
    # âœ… CuPy ë°°ì—´ì´ë©´ ê·¸ëŒ€ë¡œ
    if isinstance(p, cp.ndarray):
        return p
    # âœ… ì»¤ìŠ¤í…€ Parameter ì²˜ëŸ¼ .dataê°€ CuPy ë°°ì—´ì¸ ê²½ìš°ë§Œ .data ì‚¬ìš©
    d = getattr(p, "data", None)
    if isinstance(d, cp.ndarray):
        return d
    # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ì›ë³¸ ë¦¬í„´
    return p


def _norm(x) -> float:
    try:
        return float(cp.linalg.norm(cp.asarray(x)))
    except Exception:
        return float("nan")

def _ensure_grad_buffers(model: Sequential):
    """
    ì›Œí¬ì–´ë¼ìš´ë“œ:
      - Dense ë“±ì—ì„œ buildì‹œ dW/dbë¥¼ ë§Œë“¤ì§€ ì•Šì•˜ë‹¤ë©´ zeros_likeë¡œ ìƒì„±
      - ê·¸ ì™¸ ë ˆì´ì–´ë„ W/b, weight/bias ìŒì´ ìˆìœ¼ë©´ grad ë²„í¼ ë§Œë“¤ì–´ì¤Œ
    """
    for lyr in getattr(model, "layers", []):
        # Dense ê·œì•½ ìš°ì„ 
        if isinstance(lyr, Dense):
            if getattr(lyr, "W", None) is not None and getattr(lyr, "dW", None) is None:
                lyr.dW = cp.zeros_like(lyr.W)
            if getattr(lyr, "b", None) is not None and getattr(lyr, "db", None) is None:
                lyr.db = cp.zeros_like(lyr.b)
        # ë• íƒ€ì´í•‘: (W,dW) / (weight,dweight) / (b,db) / (bias,dbias)
        for p_name, g_name in (("W","dW"), ("weight","dweight"), ("b","db"), ("bias","dbias")):
            if hasattr(lyr, p_name) and getattr(lyr, p_name) is not None:
                if not hasattr(lyr, g_name) or getattr(lyr, g_name) is None:
                    try:
                        setattr(lyr, g_name, cp.zeros_like(getattr(lyr, p_name)))
                    except Exception:
                        pass

def main():
    print("== Optimizer update smoke (verbose) ==")
    cp.random.seed(5)

    # ---------- Config ----------
    N, Din, C = 16, 8, 3
    FORCE_NATIVE_BWD = None  # set to True/False to force, or None to keep layer default
    ACT = "none"             # "none"ìœ¼ë¡œ ë‹¨ìˆœ ê²½ë¡œë¶€í„° ê²€ì¦

    # ---------- Data ----------
    X = cp.random.standard_normal((N, Din), dtype=cp.float32)
    y = cp.random.randint(0, C, size=(N,), dtype=cp.int32)
    print(f"[data] X: shape={X.shape} dtype={X.dtype} contiguous={X.flags.c_contiguous}")
    print(f"[data] y: shape={y.shape} dtype={y.dtype}")

    # ---------- Model ----------
    dense = Dense(C, activation=ACT, use_native_bwd=(FORCE_NATIVE_BWD if FORCE_NATIVE_BWD is not None else True))
    m = Sequential(dense)
    m.build(input_shape=(N, Din))
    m.train(True)

    # grad ë²„í¼ê°€ ì—†ìœ¼ë©´ ë§Œë“¤ì–´ì¤Œ (ì›Œí¬ì–´ë¼ìš´ë“œ)
    _ensure_grad_buffers(m)

    # grad í•¸ë“¤ attach + zero
    if hasattr(m, "attach_grads"):
        m.attach_grads()
    if hasattr(m, "zero_grad"):
        m.zero_grad()

    # Dense internals quick view
    try:
        print(f"[dense] W: shape={dense.W.shape if dense.W is not None else None}, "
              f"dtype={dense.W.dtype if dense.W is not None else None}, "
              f"contig={dense.W.flags.c_contiguous if dense.W is not None else None}")
        print(f"[dense] b: shape={dense.b.shape if dense.b is not None else None}, "
              f"dtype={dense.b.dtype if dense.b is not None else None}, "
              f"contig={dense.b.flags.c_contiguous if dense.b is not None else None}")
        print(f"[dense] dW: {'None' if dense.dW is None else dense.dW.shape}, "
              f"db: {'None' if dense.db is None else dense.db.shape}")
    except Exception:
        print("[dense] failed to print W/b/dW/db details")
        traceback.print_exc()

    # ---------- Parameters discovery ----------
    params = list(m.parameters())
    print(f"[params] discovered count={len(params)}")
    for i, it in enumerate(params):
        p = _param_obj(it)
        g = _grad_obj(it)
        tag = _tag(it, default=f"param[{i}]")
        shape = getattr(p, "shape", None)
        dtype = getattr(p, "dtype", None)
        contig = getattr(p, "flags", None).c_contiguous if hasattr(p, "flags") else None
        gshape = getattr(g, "shape", None)
        gdtype = getattr(g, "dtype", None)
        print(f"  - [{i}] tag={tag:>16}  p.shape={shape} dtype={dtype} contig={contig} "
              f"| g.shape={gshape} g.dtype={gdtype}")

    if len(params) == 0:
        raise AssertionError("model has no parameters (Sequential.parameters() returned empty)")

    # ---------- Optimizer ----------
    opt = OPT_CLS(params, **OPT_KW)
    if hasattr(opt, "ensure_initialized"):
        try:
            opt.ensure_initialized()
        except Exception:
            pass
    print(f"[opt] {OPT_NAME} with hyper={OPT_KW}")

    # ---------- Snapshots ----------
    snap = [cp.asarray(_arr(p)).copy() for p in params]
    for i, s in enumerate(snap):
        print(f"[snap0] i={i} ||p||={_norm(s):.6e}")

    # ---------- Forward ----------
    print("[forward] begin")
    try:
        logits = m(X)
        print(f"[forward] logits: shape={logits.shape} dtype={logits.dtype} contig={logits.flags.c_contiguous}")
    except Exception as e:
        print("[forward] FAILED in model(X)")
        traceback.print_exc()
        raise

    # ---------- Loss ----------
    loss = SoftmaxCrossEntropy()
    try:
        L, dlogits = loss(logits, y)
        print(f"[loss] L={float(L):.6f}, dlogits: shape={getattr(dlogits,'shape',None)} "
              f"dtype={getattr(dlogits,'dtype',None)} contig={getattr(getattr(dlogits,'flags',None),'c_contiguous',None)}")
    except Exception:
        print("[loss] FAILED in SoftmaxCrossEntropy(logits, y)")
        traceback.print_exc()
        raise

    if not math.isfinite(float(L)):
        raise AssertionError("loss is not finite")

    # ---------- Backward ----------
    dlogits = cp.asarray(dlogits, dtype=cp.float32, order="C")
    try:
        g_in = m.backward(dlogits)
        print(f"[backward] ok; returned grad for input: shape={getattr(g_in,'shape',None)}")
    except Exception:
        print("[backward] FAILED in m.backward(dlogits)")
        traceback.print_exc()
        raise

    # ğŸ”§ NEW: backward ì´í›„ p.grad ì—°ê²° (í•„ìˆ˜)
    if hasattr(m, "attach_grads"):
        m.attach_grads()

    # Grad norms
    for i, it in enumerate(params):
        g = _grad_obj(it)
        print(f"[grad] param[{i}] tag={_tag(it)} ||grad||={_norm(g):.6e}")

    # ---------- Step ----------
    try:
        # support both step() and step(params)
        try:
            opt.step()
        except TypeError:
            opt.step(list(m.parameters()))  # ìƒˆ íŠœí”Œë¡œ ì „ë‹¬ (í˜¹ì‹œ ì°¸ì¡° ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œ ì˜µí‹°ë§ˆì´ì €ìš©)
        if hasattr(m, "zero_grad"):
            m.zero_grad()
    except Exception:
        print("[step] FAILED in optimizer step")
        traceback.print_exc()
        raise

    # ---------- Compare ----------
    changed = False
    total_delta = 0.0
    for i, (it, before) in enumerate(zip(params, snap)):
        after = cp.asarray(_arr(it))
        delta = _norm(after - before)
        total_delta += delta
        print(f"[compare] param[{i}] tag={_tag(it)} Î”||p||={delta:.6e} "
              f"(||before||={_norm(before):.6e} â†’ ||after||={_norm(after):.6e})")
        if not bool(cp.allclose(after, before)):
            changed = True

    print(f"[result] any_changed={changed}, total Î”||p||={total_delta:.6e}")
    if not changed:
        raise AssertionError("optimizer step should change at least one parameter")

    print("[ALL OK]")

if __name__ == "__main__":

    try:
        main()
        sys.exit(0)
    except Exception as e:
        print(f"[FAIL] {type(e).__name__}: {e}")
        sys.exit(1)

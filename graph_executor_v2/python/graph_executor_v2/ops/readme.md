OPS_LAYERS_GUIDE.md
1. ê³µí†µ êµ¬ì¡° ê°œìš”

í”„ë¡œì íŠ¸ëŠ” í¬ê²Œ ë‘ ì¸µìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤:

ops (ì €ìˆ˜ì¤€ ì—°ì‚° Wrappers)
â†’ C++/CUDA ë°”ì¸ë”©ì„ ê°ì‹¸ëŠ” íŒŒì´ì¬ ë˜í¼.
â†’ tensor pointer, shape, stride ë“±ì„ ì§ì ‘ ë‹¤ë£¨ë©° pybind11 ëª¨ë“ˆì„ í˜¸ì¶œ.

layers (ê³ ìˆ˜ì¤€ Layer ì¶”ìƒí™”)
â†’ ops ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì‹ ê²½ë§ Layerë¥¼ êµ¬ì„±.
â†’ íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜, í¸í–¥ ë“±)ì™€ ì´ˆê¸°í™”, forward/backward ë¡œì§ì„ í¬í•¨.

2. ops ë‚´ë¶€ êµ¬ì¡°
íŒŒì¼ ìœ„ì¹˜
python/graph_executor_v2/ops/
 â”£ ğŸ“œ__init__.py
 â”£ ğŸ“œcommon.py        # í…ì„œ í—¬í¼
 â”£ ğŸ“œgemm.py          # GEMM ì—°ì‚° wrapper
 â”£ ğŸ“œconv2d.py        # Conv2D ì—°ì‚° wrapper
 â”£ ğŸ“œpool2d.py        # Pool2D ì—°ì‚° wrapper
 â”£ ğŸ“œsoftmax.py       # Softmax ì—°ì‚° wrapper
 â”£ ğŸ“œcross_entropy.py # Cross-Entropy ì—°ì‚° wrapper
 â”— ...

ê³µí†µ íŒ¨í„´

pybind11 ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°

from graph_executor_v2.ops import require
g = require("gemm")   # â†’ _ops_gemm


Tensor ìƒì„±ê¸° ì‚¬ìš©

_ops_commonì—ì„œ ì œê³µë˜ëŠ” make_tensor_2d, make_tensor_4d ë“±ì„ í˜¸ì¶œ.

numpy/cupy ë°°ì—´ì„ í¬ì¸í„° + shapeë¡œ ë³€í™˜ í›„ ë°”ì¸ë”© í˜¸ì¶œ.

Forward í•¨ìˆ˜ ì •ì˜

def forward(A, B, bias=None, ...):
    tA = as_tensor_2d(A)
    tB = as_tensor_2d(B)
    tY = as_tensor_2d(Y)
    g.forward_ex(tA, tB, bias, tY, act="relu", with_bias=True)
    return Y


Backward í•¨ìˆ˜ ì •ì˜ (ì„ íƒ)

í•„ìš” ì‹œ g.backward(...) í˜¸ì¶œ.

ë°˜í™˜ê°’: dX, dW, dB ë“±.

3. layers ë‚´ë¶€ êµ¬ì¡°
íŒŒì¼ ìœ„ì¹˜
python/graph_executor_v2/layers/
 â”£ ğŸ“œbase.py        # Layer ì¶”ìƒ í´ë˜ìŠ¤
 â”£ ğŸ“œdense_gemm.py  # Dense layer
 â”£ ğŸ“œconv2d.py      # Conv2D layer
 â”£ ğŸ“œpool2d.py      # Pool2D layer
 â”£ ğŸ“œsoftmax_ce.py  # Softmax + CrossEntropy layer
 â”— ...

ê³µí†µ íŒ¨í„´

Layer ìƒì†

from .base import Layer

class Dense(Layer):
    def __init__(self, units, activation="relu", initializer="he", use_native_bwd=True):
        super().__init__()
        ...


build()

ì…ë ¥ shape ê¸°ë°˜ìœ¼ë¡œ weight/bias ìƒì„±.

self.built = True

def build(self, input_shape):
    in_dim = input_shape[-1]
    self.W = cp.random.randn(in_dim, self.units).astype(cp.float32)
    self.b = cp.zeros(self.units, dtype=cp.float32)
    self.built = True


call()

forward ì—°ì‚° ì •ì˜.

ë‚´ë¶€ì ìœ¼ë¡œ ops ëª¨ë“ˆì„ í˜¸ì¶œ.

def call(self, x):
    return gemm_ops.forward(x, self.W, self.b, act=self.activation)


backward()

í•„ìš” ì‹œ ops.backward ì‚¬ìš©.

4. ë°”ì¸ë”© â†’ ops â†’ layer ì‘ì„± ìˆœì„œ

ë°”ì¸ë”© ì½”ë“œ ì‘ì„± (src/bindings/..._pybind.cpp)

make_tensor_* ìœ í‹¸ êµ¬í˜„.

forward/backward ë°”ì¸ë”© ë“±ë¡.

ops Python wrapper ì‘ì„± (python/graph_executor_v2/ops/...py)

require("_ops_xxx") ë¡œ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°.

Tensor ë³€í™˜ í›„ ë°”ì¸ë”© í•¨ìˆ˜ í˜¸ì¶œ.

layer Python êµ¬í˜„ (python/graph_executor_v2/layers/...py)

Layer ìƒì†.

build()ì—ì„œ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”.

call()ì—ì„œ ops.forward í˜¸ì¶œ.

5. ì˜ˆì‹œ: Conv2D
ë°”ì¸ë”©
m.def("forward", [](uintptr_t x_ptr, ..., Conv2DAttrs attrs, uintptr_t stream) {
    auto st = Conv2DCudaLaunch(X, W, B, Y, attrs, stream);
    throw_if_bad(st, "Conv2DCudaLaunch");
});

ops/conv2d.py
from . import require
_g = require("conv2d")

def forward(X, W, b=None, attrs=None):
    tX = as_tensor_4d(X)
    tW = as_tensor_4d(W)
    tY = make_empty_output(...)
    _g.forward(int(X.data.ptr), list(X.shape),
               int(W.data.ptr), list(W.shape),
               int(tY.data.ptr), list(tY.shape),
               b_ptr if b is not None else None,
               attrs or _g.Conv2DAttrs())
    return tY

layers/conv2d.py
from .base import Layer
import graph_executor_v2.ops.conv2d as conv_ops

class Conv2D(Layer):
    def build(self, input_shape):
        N, C, H, W = input_shape
        self.W = cp.random.randn(self.out_channels, C, kh, kw).astype(cp.float32)
        self.b = cp.zeros(self.out_channels, dtype=cp.float32)
        self.built = True

    def call(self, x):
        return conv_ops.forward(x, self.W, self.b, self.attrs)


ğŸ‘‰ ì´ ë¬¸ì„œë§Œ ìˆìœ¼ë©´, ìƒˆë¡œìš´ ì—°ì‚°ì„ ì¶”ê°€í•  ë•Œë„

C++ ë°”ì¸ë”© â†’ ops ë˜í¼ â†’ layer êµ¬í˜„ ìˆœì„œë¡œ ë°”ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# graph_executor_v2/layers/sequential.py
from __future__ import annotations
from typing import List, Tuple, Any, Iterable, Optional, Dict
import cupy as cp

from .base import Layer

# âœ… ì¶”ê°€: ìº¡ì²˜ í”Œëœ ê¸°ë°˜ grad ì¬ë°”ì¸ë”© ìœ í‹¸
try:
    from graph_executor_v2.optim.adamw import collect_params_from_plan  # type: ignore
except Exception:
    collect_params_from_plan = None  # ëŸ°íƒ€ì„ì— ì²´í¬

CANDIDATE_PARAM_GRAD_NAMES = [
    ("W", "dW"),
    ("weight", "dweight"),
    ("b", "db"),
    ("bias", "dbias"),
]

class Sequential(Layer):
    """
    - layers: [Layer, ...]
    - call(x): ìˆœì„œëŒ€ë¡œ forward
    - backward(grad): ì—­ìˆœ ì „íŒŒ
    - parameters(): (param, grad, name) ì´í„°ë ˆì´í„°
    - zero_grad(): grad ì´ˆê¸°í™”
    - state_dict()/load_state_dict(): ì €ì¥/ë¡œë“œ
    - train()/eval(): ì„œë¸Œ ë ˆì´ì–´ê¹Œì§€ ëª¨ë“œ ì „íŒŒ
    - attach_grads(): ë ˆì´ì–´ ë‚´ë¶€ dW/db ë“±ì„ param.gradë¡œ ì—°ê²°(ê°€ëŠ¥í•œ ê²½ìš°)
    - (ì¶”ê°€) supports_capture(): ëª¨ë“  ë ˆì´ì–´ê°€ *_intoë¥¼ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸
    - (ì¶”ê°€) plan_capture(...): ìº¡ì²˜ìš© ë²„í¼/ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì‚¬ì „í• ë‹¹
    - (ì¶”ê°€) record_graph_step(...): CUDA Graph ë…¹í™” í›„ GraphExec ë°˜í™˜
    - (ì¶”ê°€) compile(...): ì „ì²´ í•™ìŠµ 1ìŠ¤í…(fwdâ†’lossâ†’bwdâ†’opt) ê·¸ë˜í”„ ì»´íŒŒì¼
    """
    def __init__(self, *layers: Layer, name: Optional[str] = None):
        super().__init__(name=name)
        self.layers: List[Layer] = list(layers)
        self.training: bool = True  # ê¸°ë³¸ í•™ìŠµ ëª¨ë“œ

    def add(self, layer: Layer) -> None:
        self.layers.append(layer)
        if self.built and self.output_shape is not None:
            ish = self.output_shape
            try:
                layer.build(ish)
                osh = layer.compute_output_shape(ish)
            except Exception:
                osh = None
            if osh is not None:
                self.output_shape = tuple(map(int, osh))

    def build(self, input_shape: Tuple[int, ...]) -> None:
        cur = tuple(map(int, input_shape))
        for lyr in self.layers:
            try:
                lyr.build(cur)
            except Exception:
                pass
            try:
                cur = tuple(map(int, lyr.compute_output_shape(cur)))
            except Exception:
                pass
        self.input_shape = tuple(map(int, input_shape))
        self.output_shape = cur if isinstance(cur, tuple) else None
        self.built = True

    # === Runtime ===
    def call(self, x: Any):
        out = x
        for lyr in self.layers:
            # ë“œë¡­ì•„ì›ƒ/BN ë“±ì€ lyr.training í”Œë˜ê·¸ë¥¼ ì°¸ì¡°í•˜ë„ë¡ êµ¬í˜„
            if hasattr(lyr, "training"):
                lyr.training = self.training
            out = lyr(out)
        return out

    def backward(self, grad_output: Any):
        g = grad_output
        for lyr in reversed(self.layers):
            g = lyr.backward(g)
        return g

    def compute_output_shape(self, input_shape: Tuple[int, ...]) -> Tuple[int, ...]:
        cur = tuple(map(int, input_shape))
        for lyr in self.layers:
            cur = lyr.compute_output_shape(cur)
        return cur

    def summary(self, indent: int = 2) -> str:
        lines = []
        pad = " " * indent
        lines.append(f"Sequential(name={self.name})")
        if self.input_shape:
            lines.append(f"{pad}Input:  {self.input_shape}")
        cur = self.input_shape
        total_params = 0
        for i, lyr in enumerate(self.layers):
            cls = lyr.__class__.__name__
            shp = None
            if cur is not None:
                try:
                    shp = lyr.compute_output_shape(cur)
                    cur = shp
                except Exception:
                    shp = "?"
                    cur = None
            # íŒŒë¼ë¯¸í„° ìˆ˜ ì§‘ê³„(ê°€ëŠ¥í•œ ê²½ìš°)
            pcount = 0
            try:
                for (p, _, _) in lyr.parameters():  # type: ignore
                    try:
                        pcount += int(p.size) if hasattr(p, "size") else int(p.size())
                    except Exception:
                        pass
            except Exception:
                pass
            total_params += pcount
            lines.append(f"{pad}[{i:02d}] {cls:>20} -> {shp}  (params={pcount})")
        if cur is not None:
            lines.append(f"{pad}Output: {cur}")
        lines.append(f"{pad}Total params: {total_params}")
        return "\n".join(lines)

    # === Training helpers ===
    def train(self, mode: bool = True):
        """model.train()/eval() í˜¸í™˜"""
        self.training = bool(mode)
        for lyr in self.layers:
            if hasattr(lyr, "training"):
                lyr.training = self.training
        return self

    def eval(self):
        return self.train(False)

    # í‘œì¤€í™”ëœ íŒŒë¼ë¯¸í„° ì¸í„°í˜ì´ìŠ¤
    def parameters(self) -> Iterable[Tuple[Any, Any, str]]:
        for idx, lyr in enumerate(self.layers):
            lname = f"{lyr.__class__.__name__}:{idx}"
            if hasattr(lyr, "parameters") and callable(getattr(lyr, "parameters")):
                # (param, grad, name) or (param, grad)
                for t in lyr.parameters():  # type: ignore
                    if isinstance(t, tuple) and len(t) == 3:
                        yield t  # (p, g, name)
                    elif isinstance(t, tuple) and len(t) == 2:
                        p, g = t
                        yield (p, g, lname)
                continue
            # íœ´ë¦¬ìŠ¤í‹± ìŠ¤ìº” (W/dW, b/db ë“±)
            for p_name, g_name in CANDIDATE_PARAM_GRAD_NAMES:
                if hasattr(lyr, p_name) and hasattr(lyr, g_name):
                    p = getattr(lyr, p_name)
                    g = getattr(lyr, g_name)
                    yield (p, g, f"{lname}.{p_name}")

    def zero_grad(self):
        """param.grad ë° ë ˆì´ì–´ ë‚´ë¶€ dW/db ë“±ì„ 0ìœ¼ë¡œ."""
        # param.grad ì´ˆê¸°í™”
        for (p, _, _) in self.parameters():
            g = getattr(p, "grad", None)
            if g is not None:
                try:
                    g[...] = 0
                except Exception:
                    try:
                        if hasattr(g, "zero_"):
                            g.zero_()
                        else:
                            setattr(p, "grad", None)
                    except Exception:
                        # cupy.ndarray ëŠ” ì„ì˜ ì†ì„± ë¶€ì—¬ê°€ ì•ˆ ë  ìˆ˜ ìˆìŒ â†’ ì¡°ìš©íˆ íŒ¨ìŠ¤
                        pass
        # ë ˆì´ì–´ ë‚´ë¶€ ê·¸ë¼ë“œ ìºì‹œ ì´ˆê¸°í™”
        for lyr in self.layers:
            # ì‚¬ìš©ì ì •ì˜ zero_gradê°€ ìˆìœ¼ë©´ ìš°ì„ 
            if hasattr(lyr, "zero_grad") and callable(getattr(lyr, "zero_grad")):
                try:
                    lyr.zero_grad()  # type: ignore
                except Exception:
                    pass
                continue
            # ì—†ìœ¼ë©´ íœ´ë¦¬ìŠ¤í‹±
            for _, g_name in CANDIDATE_PARAM_GRAD_NAMES:
                if hasattr(lyr, g_name):
                    g = getattr(lyr, g_name)
                    try:
                        g[...] = 0
                    except Exception:
                        try:
                            if hasattr(g, "zero_"):
                                g.zero_()
                            else:
                                setattr(lyr, g_name, None)
                        except Exception:
                            pass

    def attach_grads(self):
        """
        ë ˆì´ì–´ ë‚´ë¶€ dW/db ë“±ì„ param.gradì— ì—°ê²°.
        - Optimizerê°€ param.gradë¥¼ ì½ëŠ” êµ¬ì¡°(AdamW/SGD ë“±) ì§€ì›.
        - ì—­ì „íŒŒ ì§í›„ í˜¸ì¶œ.
        - cupy.ndarrayëŠ” ì„ì˜ ì†ì„± ë¶€ì—¬ ì‹¤íŒ¨ ê°€ëŠ¥ â†’ ì‹¤íŒ¨ ì‹œ ì¡°ìš©íˆ ìŠ¤í‚µ
        """
        for (p, g, _) in self.parameters():
            if g is not None:
                try:
                    setattr(p, "grad", g)
                except Exception:
                    # ì˜µí‹°ë§ˆì´ì €ê°€ (param, grad) íŠœí”Œì„ ì§ì ‘ ë°›ëŠ”ë‹¤ë©´ ì´ ë‹¨ê³„ê°€ ë¶ˆí•„ìš”
                    pass

    # === ì €ì¥/ë¡œë“œ ===
    def state_dict(self) -> dict:
        """
        ë ˆì´ì–´ê°€ state_dict() ì œê³µí•˜ë©´ ìœ„ì„.
        ì—†ìœ¼ë©´ (W,b ë“±) íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•´ ì €ì¥.
        """
        sd = {"name": self.name, "layers": []}
        for lyr in self.layers:
            entry = {"class": lyr.__class__.__name__}
            if hasattr(lyr, "state_dict"):
                try:
                    entry["state"] = lyr.state_dict()  # type: ignore
                except Exception:
                    entry["state"] = None
            else:
                # íœ´ë¦¬ìŠ¤í‹± íŒŒë¼ë¯¸í„° ìŠ¤ëƒ…ìƒ·
                params = {}
                for p_name, _ in CANDIDATE_PARAM_GRAD_NAMES:
                    if hasattr(lyr, p_name):
                        params[p_name] = getattr(lyr, p_name)
                entry["params"] = params
            sd["layers"].append(entry)
        return sd

    def load_state_dict(self, sd: dict):
        """
        ì €ì¥í•œ ìˆœì„œ/íƒ€ì…ì´ ë™ì¼í•˜ë‹¤ëŠ” ê°€ì •.
        """
        assert len(sd.get("layers", [])) == len(self.layers), "layer count mismatch"
        for lyr, entry in zip(self.layers, sd["layers"]):
            if hasattr(lyr, "load_state_dict") and entry.get("state", None) is not None:
                try:
                    lyr.load_state_dict(entry["state"])  # type: ignore
                    continue
                except Exception:
                    pass
            # íœ´ë¦¬ìŠ¤í‹± ë¡œë“œ
            params = entry.get("params", {})
            for k, v in params.items():
                if hasattr(lyr, k):
                    try:
                        getattr(lyr, k)[...] = v
                    except Exception:
                        setattr(lyr, k, v)
        return self

    # =========================
    # ===== Graph Capture =====
    # =========================
    def supports_capture(self) -> bool:
        """
        ëª¨ë“  ë ˆì´ì–´ê°€ forward_into/backward_into ë¥¼ ì œê³µí•˜ëŠ”ì§€ ê²€ì‚¬.
        (pass-through ë ˆì´ì–´ëŠ” forward_intoë§Œ ì œê³µí•´ë„ ë¬´ë°©í•˜ì§€ë§Œ,
         í•™ìŠµ ìº¡ì²˜ë¥¼ ìœ„í•´ì„  ì—­ì „íŒŒ ë ˆì´ì–´ë“¤ë„ backward_into í•„ìš”)
        """
        ok = True
        for lyr in self.layers:
            f_ok = hasattr(lyr, "forward_into") and callable(getattr(lyr, "forward_into"))
            b_ok = hasattr(lyr, "backward_into") and callable(getattr(lyr, "backward_into"))
            ok = ok and f_ok and b_ok
        return ok

    def plan_capture(
        self,
        input_shape: Tuple[int, ...],
        *,
        loss_kind: str = "softmax_ce",
        lt_bytes: int = (8 << 20),  # ì˜ˆ: 8MB
    ) -> Dict[str, Any]:
        """
        ìº¡ì²˜ ì‹¤í–‰ì— í•„ìš”í•œ ì‚¬ì „ ë²„í¼/ì›Œí¬ìŠ¤í˜ì´ìŠ¤ë¥¼ í• ë‹¹í•˜ê³  shapeë¥¼ ì •ë¦¬.
        í˜„ì¬ëŠ” Dense ë ˆì´ì–´(= GEMM ê¸°ë°˜) ìœ„ì£¼ë¡œ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ë¥¼ ì¤€ë¹„.
        ë°˜í™˜: dict { 'buffers': ..., 'workspaces': ..., 'shapes': ... }
        """
        from graph_executor_v2.ops import gemm as gops  # ì§€ì—° ì„í¬íŠ¸
        cur = tuple(map(int, input_shape))
        bufs: Dict[str, Any] = {"fwd": [], "bwd": []}
        wss: Dict[str, Any] = []
        shapes: Dict[str, Any] = {"per_layer": []}

        # ìˆœì „íŒŒ ì¶œë ¥/í”„ë¦¬ì•¡í‹°ë² ì´ì…˜ ì €ì¥ ì—¬ë¶€ íŒë‹¨
        for idx, lyr in enumerate(self.layers):
            oname = f"L{idx}:{lyr.__class__.__name__}"
            try:
                out_shp = tuple(map(int, lyr.compute_output_shape(cur)))
            except Exception:
                out_shp = None
            shapes["per_layer"].append((oname, {"in": cur, "out": out_shp}))
            # Denseì¼ ê²½ìš°: Z ë²„í¼ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ(activation != 'none')
            need_z = getattr(lyr, "activation", "none") != "none"
            # ìˆœì „íŒŒ out ë²„í¼
            if out_shp is not None:
                y = cp.empty(out_shp, dtype=cp.float32)
            else:
                raise RuntimeError(f"cannot infer output shape for {oname}")
            # Z(ì˜µì…˜)
            z = cp.empty(out_shp, dtype=cp.float32) if need_z else None

            bufs["fwd"].append({"y": y, "z": z, "name": oname})

            # ì—­ì „íŒŒ ë²„í¼ (gA, gW, gB ë“±) â€” Dense ê¸°ì¤€ íœ´ë¦¬ìŠ¤í‹±
            # ë ˆì´ì–´ ë‚´ë¶€ì—ì„œ ê²€ì¦í•˜ë¯€ë¡œ ì—¬ê¸°ì„  ëª¨ì–‘ë§Œ ì¶”ì •
            if hasattr(lyr, "W") and hasattr(lyr, "b"):
                W = getattr(lyr, "W")
                in_dim, units = int(W.shape[0]), int(W.shape[1])
                gA = cp.empty(cur, dtype=cp.float32)
                gW = cp.empty_like(W)
                gB = cp.empty((1, units), dtype=cp.float32)
                bufs["bwd"].append({"gA": gA, "gW": gW, "gB": gB, "name": oname})
                # GEMM ì›Œí¬ìŠ¤í˜ì´ìŠ¤
                ws = gops.ensure_workspaces(cur[0], units, lt_bytes=lt_bytes)
                wss.append({"work": ws, "name": oname})
            else:
                # íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” ë ˆì´ì–´(ì˜ˆ: í™œì„±í™”, reshape ë“±)
                gA = cp.empty(cur, dtype=cp.float32)
                bufs["bwd"].append({"gA": gA, "gW": None, "gB": None, "name": oname})
                wss.append({"work": None, "name": oname})

            # ë‹¤ìŒ ì…ë ¥ shape ê°±ì‹ 
            cur = out_shp

        # ìµœì¢… ë¡œì§“/ì†ì‹¤ìš© dY ë²„í¼ (softmax CE ê°€ì •)
        if loss_kind == "softmax_ce":
            dY = cp.empty(cur, dtype=cp.float32)
            bufs["loss"] = {"dY": dY, "out_shape": cur}
        else:
            bufs["loss"] = {"dY": None, "out_shape": cur}

        return {"buffers": bufs, "workspaces": wss, "shapes": shapes}


    def record_graph_step(
        self,
        X: cp.ndarray,
        y: Any,
        *,
        loss_fn,
        optimizer_step_fn,
        capture_plan: Dict[str, Any],
        stream: Optional[cp.cuda.Stream] = None,
    ):
        """
        1 ìŠ¤í… í•™ìŠµì„ CUDA Graphë¡œ ë…¹í™”í•˜ê³  GraphExec ìœ ì‚¬ ê°ì²´ë¥¼ ë°˜í™˜.
        - CUDA Graph ìº¡ì²˜ APIê°€ í™˜ê²½ì— ì—†ìœ¼ë©´ ë™ì¼ ë™ì‘ì˜ í´ë°± ê°ì²´ë¥¼ ë°˜í™˜(launchë§Œ ì œê³µ).
        """
        if stream is None:
            stream = cp.cuda.Stream(non_blocking=True)

        bufs = capture_plan["buffers"]
        fwd_bufs = bufs["fwd"]
        bwd_bufs = bufs["bwd"]
        dY = bufs["loss"]["dY"]
        workspaces = capture_plan["workspaces"]

        # ---- ì›Œë°ì—… 1íšŒ(ëª¨ì–‘/íƒ€ì…/ìŠ¤íŠ¸ë¦¼ ê³ ì •) ----
        with stream:
            cur = X
            # FWD
            for idx, lyr in enumerate(self.layers):
                ybuf = fwd_bufs[idx]["y"]
                zbuf = fwd_bufs[idx]["z"]
                lyr.forward_into(cur, out=ybuf, z_out=zbuf, stream=stream.ptr)
                cur = ybuf
            # LOSS (âš ï¸ í˜¸ìŠ¤íŠ¸ ë³µì‚¬ ê¸ˆì§€)
            loss_dev, dY_tmp = loss_fn.forward(cur, y, return_scalar=False)
            if dY is not None:
                dY[...] = dY_tmp

            # âœ… BWD ì „ì— ìº¡ì²˜ grad ë²„í¼ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™” (ëˆ„ì  ë°©ì§€)
            for i in range(len(self.layers)):
                b = bwd_bufs[i]
                ga = b.get("gA", None)
                gw = b.get("gW", None)
                gb = b.get("gB", None)
                if ga is not None: ga.fill(0)
                if gw is not None: gw.fill(0)
                if gb is not None: gb.fill(0)

            # BWD (ì—­ìˆœ)
            g = dY if dY is not None else dY_tmp
            for ridx, lyr in enumerate(reversed(self.layers)):
                i = len(self.layers) - 1 - ridx
                b = bwd_bufs[i]
                ws = workspaces[i]["work"]
                if b["gW"] is not None:
                    lyr.backward_into(
                        g,
                        gA_out=b["gA"], gW_out=b["gW"], gB_out=b["gB"],
                        work_dZ=(ws.dZ if ws is not None else None),
                        lt_workspace=(ws.lt_ws if (ws is not None and ws.lt_ws is not None) else None),
                        stream=stream.ptr
                    )
                else:
                    lyr.backward_into(  # type: ignore
                        g, gA_out=b["gA"],
                        work_dZ=None, lt_workspace=None, stream=stream.ptr
                    )
                g = b["gA"]
            # OPT
            optimizer_step_fn()

        # ---- ê·¸ë˜í”„ ìº¡ì²˜ ì§€ì› ì—¬ë¶€ í™•ì¸ ----
        _has_capture_stream = hasattr(cp.cuda, "graph") and hasattr(cp.cuda.graph, "capture_stream")

        if _has_capture_stream:
            # ========== ì§„ì§œ CUDA Graph ê²½ë¡œ ==========
            with stream:
                with cp.cuda.graph.capture_stream(stream) as cap:
                    cur = X
                    # FWD
                    for idx, lyr in enumerate(self.layers):
                        ybuf = fwd_bufs[idx]["y"]
                        zbuf = fwd_bufs[idx]["z"]
                        lyr.forward_into(cur, out=ybuf, z_out=zbuf, stream=stream.ptr)
                        cur = ybuf
                    # LOSS (âš ï¸ í˜¸ìŠ¤íŠ¸ ë³µì‚¬ ê¸ˆì§€)
                    loss_dev, dY_tmp = loss_fn.forward(cur, y, return_scalar=False)
                    if dY is not None:
                        dY[...] = dY_tmp
                        g_in = dY
                    else:
                        g_in = dY_tmp

                    # âœ… BWD ì „ì— ìº¡ì²˜ grad ë²„í¼ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™” (ëˆ„ì  ë°©ì§€)
                    for i in range(len(self.layers)):
                        b = bwd_bufs[i]
                        ga = b.get("gA", None)
                        gw = b.get("gW", None)
                        gb = b.get("gB", None)
                        if ga is not None: ga.fill(0)
                        if gw is not None: gw.fill(0)
                        if gb is not None: gb.fill(0)

                    # BWD
                    for ridx, lyr in enumerate(reversed(self.layers)):
                        i = len(self.layers) - 1 - ridx
                        b = bwd_bufs[i]
                        ws = workspaces[i]["work"]
                        if b["gW"] is not None:
                            lyr.backward_into(
                                g_in,
                                gA_out=b["gA"], gW_out=b["gW"], gB_out=b["gB"],
                                work_dZ=(ws.dZ if ws is not None else None),
                                lt_workspace=(ws.lt_ws if (ws is not None and ws.lt_ws is not None) else None),
                                stream=stream.ptr
                            )
                        else:
                            lyr.backward_into(  # type: ignore
                                g_in, gA_out=b["gA"],
                                work_dZ=None, lt_workspace=None, stream=stream.ptr
                            )
                        g_in = b["gA"]
                    # OPT
                    optimizer_step_fn()

            graph = cap.graph
            gexec = graph.instantiate()
            return gexec

        # ========== í´ë°±: ê·¸ë˜í”„ ë¯¸ì§€ì› ì‹œ GraphExec ìœ ì‚¬ ê°ì²´ ë°˜í™˜ ==========
        def _one_step():
            # ì£¼ì˜: ìŠ¤íŠ¸ë¦¼/ë²„í¼/shapeëŠ” ì´ë¯¸ ê³ ì •ë˜ì–´ ìˆë‹¤ê³  ê°€ì •(ì›Œë°ì—… ë™ì¼ ìˆœì„œ)
            cur = X
            for idx, lyr in enumerate(self.layers):
                ybuf = fwd_bufs[idx]["y"]
                zbuf = fwd_bufs[idx]["z"]
                lyr.forward_into(cur, out=ybuf, z_out=zbuf, stream=stream.ptr)
                cur = ybuf
            loss_dev, dY_tmp = loss_fn.forward(cur, y, return_scalar=False)
            g_in = dY if dY is not None else dY_tmp

            # âœ… BWD ì „ì— ìº¡ì²˜ grad ë²„í¼ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™” (ëˆ„ì  ë°©ì§€)
            for i in range(len(self.layers)):
                b = bwd_bufs[i]
                ga = b.get("gA", None)
                gw = b.get("gW", None)
                gb = b.get("gB", None)
                if ga is not None: ga.fill(0)
                if gw is not None: gw.fill(0)
                if gb is not None: gb.fill(0)

            for ridx, lyr in enumerate(reversed(self.layers)):
                i = len(self.layers) - 1 - ridx
                b = bwd_bufs[i]
                ws = workspaces[i]["work"]
                if b["gW"] is not None:
                    lyr.backward_into(
                        g_in,
                        gA_out=b["gA"], gW_out=b["gW"], gB_out=b["gB"],
                        work_dZ=(ws.dZ if ws is not None else None),
                        lt_workspace=(ws.lt_ws if (ws is not None and ws.lt_ws is not None) else None),
                        stream=stream.ptr
                    )
                else:
                    lyr.backward_into(  # type: ignore
                        g_in, gA_out=b["gA"],
                        work_dZ=None, lt_workspace=None, stream=stream.ptr
                    )
                g_in = b["gA"]
            optimizer_step_fn()

        class _PseudoGraphExec:
            """CUDA Graphì´ ì—†ì„ ë•Œë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ëŒ€ì²´. launch(stream_ptr)ë¥¼ í‰ë‚´ë‚¸ë‹¤."""
            def __init__(self, step_fn, stream_obj):
                self._step_fn = step_fn
                self._stream = stream_obj
            def launch(self, stream_ptr=None):
                # stream_ptrëŠ” ë¬´ì‹œ(ì´ë¯¸ self._streamë¡œ ê³ ì •)
                with self._stream:
                    self._step_fn()

        return _PseudoGraphExec(_one_step, stream)

    # =========================
    # ======== Compile ========
    # =========================
    def compile(
        self,
        input_shape,
        *,
        loss,
        optimizer,                   # AdamWOpt ë“±: step_into() ë³´ìœ  (ê·¸ë¦¬ê³  rebind_grads ì§€ì›)
        lt_bytes: int = (8 << 20),
        stream: Optional[cp.cuda.Stream] = None,
    ) -> "TrainGraph":
        """
        ì „ì²´ í•™ìŠµ 1ìŠ¤í…(fwdâ†’lossâ†’bwdâ†’opt)ì„ CUDA Graphë¡œ ìº¡ì²˜í•˜ê³ 
        ê³ ì • ì…ì¶œë ¥ ë²„í¼/ê·¸ë˜í”„ ì‹¤í–‰ìë¥¼ ë¬¶ì€ TrainGraphë¥¼ ë°˜í™˜.
        ë³€ê²½ì :
          - IO ë²„í¼ë¥¼ zerosë¡œ ì´ˆê¸°í™”(ë¼ë²¨ out-of-range ë°©ì§€)
          - ìº¡ì²˜ í”Œëœ ìƒì„± ì§í›„ optimizerì˜ grad í¬ì¸í„°ë¥¼ gW/gBë¡œ ì¬ë°”ì¸ë”©
        """
        if not self.built:
            self.build(tuple(map(int, input_shape)))

        assert self.supports_capture(), "All layers must implement forward_into/backward_into for capture"

        bs, in_dim = int(input_shape[0]), int(input_shape[1])

        # âœ… ì•ˆì „í•˜ê²Œ zerosë¡œ ì´ˆê¸°í™” (ë¼ë²¨/ì…ë ¥ ì“°ë ˆê¸°ê°’ ë°©ì§€)
        X_buf = cp.zeros((bs, in_dim), dtype=cp.float32)
        y_buf = cp.zeros((bs,),        dtype=cp.int32)   # í´ë˜ìŠ¤ 0ìœ¼ë¡œ ì´ˆê¸°í™” â†’ í•­ìƒ ìœ íš¨

        # ìº¡ì²˜ í”Œëœ
        plan = self.plan_capture(tuple(map(int, input_shape)), loss_kind="softmax_ce", lt_bytes=lt_bytes)

        # ğŸ”— ì˜µí‹°ë§ˆì´ì € grad í¬ì¸í„°ë¥¼ ìº¡ì²˜ ë²„í¼(gW/gB)ë¡œ ì¬ë°”ì¸ë”©
        if hasattr(optimizer, "rebind_grads") and collect_params_from_plan is not None:
            cap_triplets = collect_params_from_plan(self, plan)
            optimizer.rebind_grads(cap_triplets)
        else:
            # (í´ë°±) rebind ë¶ˆê°€ ì‹œ, record_graph_step ë‚´ë¶€ì—ì„œ plan gW/gB â†’ layer.dW/db ë³µì‚¬ í•„ìš”
            # ê¶Œì¥: AdamWOptì— rebind_gradsë¥¼ êµ¬í˜„í•˜ê³  ì—¬ê¸°ì„œ ì¬ë°”ì¸ë”©í•˜ì„¸ìš”.
            pass

        # ìº¡ì²˜ìš© ìŠ¤íŠ¸ë¦¼ ê³ ì •
        if stream is None:
            stream = cp.cuda.Stream(non_blocking=True)

        # ê·¸ë˜í”„ ë…¹í™” (optimizer.step_intoë¥¼ ì½œë°±ìœ¼ë¡œ)
        gexec = self.record_graph_step(
            X_buf, y_buf,
            loss_fn=loss,
            optimizer_step_fn=optimizer.step_into,
            capture_plan=plan,
            stream=stream
        )

        # ë§ˆì§€ë§‰ ë ˆì´ì–´ ì¶œë ¥ ë²„í¼ë¥¼ ë…¸ì¶œ(ë¡œê¹…/ê²€ì¦ìš©)
        io = {
            "X": X_buf,
            "y": y_buf,
            "logits": plan["buffers"]["fwd"][-1]["y"],
        }
        return TrainGraph(gexec, io, stream)


class TrainGraph:
    """
    CUDA Graph ì‹¤í–‰ í•¸ë“¤ + ê³ ì • IO ë²„í¼ë¥¼ ë³´ê´€.
      - set_batch(X, y): ê³ ì •ëœ ë””ë°”ì´ìŠ¤ ë²„í¼ì— ë³µì‚¬(í¬ì¸í„° ë¶ˆë³€)
      - launch(): ìº¡ì²˜ëœ ê·¸ë˜í”„ 1ìŠ¤í… ì‹¤í–‰
      - logits: ë§ˆì§€ë§‰ ë ˆì´ì–´ FWD ì¶œë ¥ ë²„í¼(ê³ ì • í¬ì¸í„°)
    """
    def __init__(self, gexec, io, stream):
        self._gexec = gexec
        self._io = io
        self._stream = stream

    @property
    def logits(self):
        return self._io["logits"]

    @property
    def X_buf(self):
        return self._io["X"]

    @property
    def y_buf(self):
        return self._io["y"]

    def set_batch(self, X_dev, y_dev):
        # CPU â†’ GPUë„ í—ˆìš©: cupy.asarrayë¡œ ë³µì‚¬
        xb = self._io["X"]; yb = self._io["y"]
        xb[...] = cp.asarray(X_dev, dtype=xb.dtype)
        yb[...] = cp.asarray(y_dev, dtype=yb.dtype)

    def launch(self):
        self._gexec.launch(self._stream.ptr)

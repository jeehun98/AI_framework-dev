# File: python/graph_executor_v2/layers/sequential.py
from __future__ import annotations
from typing import (
    List, Tuple, Any, Iterable, Optional, Dict, Sequence, TYPE_CHECKING
)
import cupy as cp

# ============================================================================
# ì´ íŒŒì¼ì˜ ëª©ì 
# ----------------------------------------------------------------------------
# - ê³ ìˆ˜ì¤€ Sequential ì»¨í…Œì´ë„ˆ:
#     * Eager ê²½ë¡œ: call()/backward() ë¡œ ì¦‰ì‹œ ì‹¤í–‰
#     * Graph Capture ê²½ë¡œ:
#         - ì •ì (Static): compile() â†’ one_step()
#         - ë™ì (Dynamic): one_step_dynamic()  (If/Repeat/EarlyExit ë“± ë¶„ê¸° í¬í•¨)
#
# - ì™¸ë¶€ ëª¨ë“ˆ ì—°ê²°:
#     * graph.capture_plan:
#         - make_plan_for_sequential(): ì •ì  ì „ì²´ ëª¨ë¸ í”Œëœ
#         - make_plan_for_path(): ë™ì  "í‰íƒ„í™”ëœ ê²½ë¡œ" ì „ìš© í”Œëœ
#         - advance_dropout(): ë°˜ë³µì‹œ ì‹œë“œ/ë§ˆìŠ¤í¬ ì „ì§„
#     * graph.graph_exec:
#         - record_step_graph(): fwdâ†’lossâ†’bwdâ†’opt 1 stepì„ CUDA Graphë¡œ ìº¡ì²˜
#         - TrainGraph: set_batch()/launch() ë¡œ ì¬ìƒ(replay)
#     * graph.graph_executor:
#         - GraphSignature/GraphKey/MultiGraphPool, graph_pool ì¸ìŠ¤í„´ìŠ¤
#           (ë™ì  ê²½ë¡œë³„ TrainGraph ìºì‹œ/ì¬ì‚¬ìš©ì— ì“°ì„)
#
# - NVTX íƒœê¹…:
#     * íƒ€ì„ë¼ì¸ ë¶„ì„ì„ ìœ„í•œ í†µì¼ëœ ë„¤ì´ë° ì‚¬ìš©
#     * [CAPTURE][static] / [REPLAY][static] / [DYN] ... ë“±
#
# - í–¥í›„ í™•ì¥(ì„¤ê³„ ì—¬ì§€):
#     * Execution Planner(í† í´ë¡œì§€â†’ìŠ¤íŠ¸ë¦¼/ì´ë²¤íŠ¸ ìŠ¤ì¼€ì¤„) ì‚½ì… ì§€ì :
#         - ì •ì : compile()ì—ì„œ make_plan_for_sequential(...) ì§í›„
#         - ë™ì : _get_or_capture_dynamic_entry()ì—ì„œ make_plan_for_path(...) ì§í›„
#     * Graph Runtime(Allocator/RNG/Stream/Tape í†µí•©) ì£¼ë„ ìº¡ì²˜:
#         - graph_exec.record_step_graph(...) ë‚´ë¶€
# ============================================================================

from graph_executor_v2.graph.capture_plan import (
    make_plan_for_sequential,
    make_plan_for_path,
    advance_dropout,
)
from graph_executor_v2.graph.graph_exec import record_step_graph, TrainGraph
from graph_executor_v2.optim.rebind import try_rebind_grads
from graph_executor_v2.graph.rewriter import run as rewrite

from .base import Layer

# ğŸ”½ íŒ¨í„´ íŒ¨ìŠ¤ (í˜„ì¬ no-op) â€” ì´í›„ ìµœì í™”/í“¨ì „ ì¶”ê°€ ì‹œ í™œì„±í™”
from graph_executor_v2.graph.pattern_registry import run_patterns

import inspect
import time

# ===== NVTX (optional) =====
# í†µì¼ëœ ë„¤ì´ë°ìœ¼ë¡œ íƒ€ì„ë¼ì¸ ë¶„ì„ì„ ì‰½ê²Œ í•˜ê¸° ìœ„í•´ ë˜í¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
try:
    from graph_executor_v2.backends.cuda.ops.gemm.detail.nvtx_shim import nvtx_range  # type: ignore
except Exception:
    class _DummyNvtx:
        def __call__(self, *_a, **_k):
            class _Ctx:
                def __enter__(self): return None
                def __exit__(self, *args): return False
            return _Ctx()
    nvtx_range = _DummyNvtx()  # type: ignore

# âœ… ëŸ°íƒ€ì„ ì„í¬íŠ¸ ìš°ì„ : ì‹¤ì œ í´ë˜ìŠ¤ ë¡œë“œ ì‹¤íŒ¨ì‹œ ìŠ¤í…ìœ¼ë¡œ í´ë°±
#    - ë™ì  ê²½ë¡œ í‰íƒ„í™”(_linearize_path)ì—ì„œ If/Repeat/EarlyExitë¥¼ "ë• íƒ€ì´í•‘"ìœ¼ë¡œë§Œ ì‹ë³„í•˜ë¯€ë¡œ
#      ì´ ì„í¬íŠ¸ê°€ ì‹¤íŒ¨í•´ë„ ê¸°ëŠ¥ìƒ ë¬¸ì œëŠ” ì—†ìŒ(ì²´í¬ëŠ” getattr ê¸°ë°˜).
try:
    from graph_executor_v2.layers.conditional import If, Repeat, EarlyExit  # ì‹¤ì œ ì»¨íŠ¸ë¡¤ ë ˆì´ì–´
except Exception:
    class _Missing: ...
    If = Repeat = EarlyExit = _Missing  # fallback stubs

# ============================
# Typing-only imports & stubs
# ============================
if TYPE_CHECKING:
    from graph_executor_v2.graph.graph_executor import (
        GraphSignature, GraphKey, MultiGraphPool
    )
else:
    # ëŸ°íƒ€ì„ì— íƒ€ì…ì´ ì—†ë”ë¼ë„ íŒŒì¼ì€ ë™ì‘í•´ì•¼ í•˜ë¯€ë¡œ Anyë¡œ í´ë°±
    from typing import Any as _AnyType
    GraphSignature = _AnyType  # type: ignore[assignment]
    GraphKey = _AnyType        # type: ignore[assignment]
    MultiGraphPool = _AnyType  # type: ignore[assignment]

# ëŸ°íƒ€ì„ ì¸ìŠ¤í„´ìŠ¤ ë¡œë”© (ì—†ì–´ë„ ë™ì‘í•˜ë„ë¡ í´ë°± ì¤€ë¹„)
# - ë™ì  ê²½ë¡œ ê·¸ë˜í”„ ìºì‹œ(í’€)ê°€ ì¡´ì¬í•˜ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ë¡œì»¬ dictë¡œ ëŒ€ì²´
try:
    from graph_executor_v2.graph.graph_executor import graph_pool  # type: ignore
except Exception:
    graph_pool = None  # type: ignore

# í´ë°±: í”„ë¡œì„¸ìŠ¤ ë‚´ ê°„ë‹¨í•œ ìºì‹œ(dict) + LRU
_FALLBACK_POOL: Dict[Any, Any] = {}

# parameters()ì—ì„œ (p, g) ìë™ íƒìƒ‰ ì‹œ ì‚¬ìš©í•˜ëŠ” í›„ë³´ ì†ì„±ëª…ë“¤
CANDIDATE_PARAM_GRAD_NAMES = [
    ("W", "dW"),
    ("weight", "dweight"),
    ("b", "db"),
    ("bias", "dbias"),
]

# ë™ì  ê²½ë¡œ í˜¸í™˜ìš©: ì˜› graph_execì—ë„ ë™ì‘í•˜ë„ë¡ í•˜ëŠ” í”„ë¡ì‹œ
class _ModelLayersProxy:
    """modelì˜ ë‹¤ë¥¸ ì†ì„±ì€ ê·¸ëŒ€ë¡œ ìœ„ì„í•˜ê³ , layersë§Œ path_layersë¡œ ë°”ê¿”ì¹˜ê¸°.

    - ì¼ë¶€ record_step_graph ë²„ì „ì´ layers_overrideë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì‚¬ìš©.
    - self._baseì— ëª¨ë“  ì ‘ê·¼ì„ ìœ„ì„í•˜ë˜, 'layers' ì ‘ê·¼/ì„¤ì •ë§Œ overrideí•œë‹¤.
    """
    def __init__(self, base, layers):
        self._base = base
        self.layers = list(layers)

    def __getattr__(self, name):
        if name == "layers":
            return self.layers
        return getattr(self._base, name)

    def __setattr__(self, name, value):
        if name in ("_base", "layers"):
            object.__setattr__(self, name, value)
        else:
            setattr(self._base, name, value)


class Sequential(Layer):
    """ê³ ìˆ˜ì¤€ ìˆœì°¨ ëª¨ë¸ ì»¨í…Œì´ë„ˆ.

    â–¶ ì§€ì› ëª¨ë“œ
      - Eager: call()/backward()
      - Graph(ì •ì ): compile() â†’ one_step()
      - Graph(ë™ì ): one_step_dynamic()  (If/Repeat/EarlyExit í¬í•¨ ê²½ë¡œë³„ ìº¡ì²˜/ìºì‹œ)

    â–¶ ì™¸ë¶€ ì—°ë™
      - capture_plan: make_plan_for_*(), advance_dropout()
      - graph_exec: record_step_graph(), TrainGraph
      - graph_executor: GraphKey/GraphSignature/graph_pool
    """
    # í´ë°± í’€ ìƒí•œ/LRU ì œì–´ìš©
    _FALLBACK_POOL_MAX = 8

    def __init__(self, *layers: Layer, name: Optional[str] = None):
        super().__init__(name=name)
        self.layers: List[Layer] = list(layers)
        self.training: bool = True
        self._tg: Optional[TrainGraph] = None
        self._loss_buf: Optional[cp.ndarray] = None
        self._stream: Optional[cp.cuda.Stream] = None
        self._pool_ticks: int = 0
        # === NEW: local telemetry counters ===
        self._tm = {
            "capture_count": 0,
            "replay_count": 0,
            "pool_hit": 0,
            "pool_miss": 0,
            "pool_put": 0,
            "pool_evict_fallback": 0,
        }

    def _tick(self) -> int:
        self._pool_ticks += 1
        return self._pool_ticks

    # -------------------------------------------------------------------------
    # êµ¬ì„±/ë¹Œë“œ
    # -------------------------------------------------------------------------
    def add(self, layer: Layer) -> None:
        """ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ê³ , ì´ë¯¸ ë¹Œë“œëœ ìƒíƒœë¼ë©´ ê°„ë‹¨íˆ ì¶œë ¥ shapeë¥¼ ì¶”ì  ê°±ì‹ ."""
        self.layers.append(layer)
        if self.built and self.output_shape is not None:
            ish = self.output_shape
            try:
                layer.build(ish)
                osh = layer.compute_output_shape(ish)
            except Exception:
                osh = None
            if osh is not None:
                self.output_shape = tuple(map(int, osh))

    def build(
        self,
        input_shape: Tuple[int, ...],
        *,
        strict: bool = True,
        verify_output: bool = True
    ) -> None:
        """ëª¨ë“  í•˜ìœ„ ë ˆì´ì–´ì— ëŒ€í•´ build/compute_output_shapeë¥¼ ìˆœì°¨ ìˆ˜í–‰.

        - strict=True: ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ ì˜ˆì™¸ ë°œìƒ ì‹œ ì¦‰ì‹œ ì‹¤íŒ¨
        - verify_output=True: ì „ì²´ ë¹Œë“œ ì¢…ë£Œ í›„ ê²°ê³¼ ê²€ì¦/ì˜¤ë¥˜ ë¦¬í¬íŠ¸
        """
        cur = tuple(map(int, input_shape))
        errors = []

        for i, lyr in enumerate(self.layers):
            lname = f"{lyr.__class__.__name__}:{i}"
            try:
                if hasattr(lyr, "build"):
                    lyr.build(cur)
            except Exception as e:
                msg = f"[Sequential.build] build failed at {lname} with in_shape={cur}: {e}"
                if strict:
                    raise RuntimeError(msg) from e
                errors.append(msg)
            try:
                if hasattr(lyr, "compute_output_shape"):
                    cur = tuple(map(int, lyr.compute_output_shape(cur)))
            except Exception as e:
                msg = f"[Sequential.build] compute_output_shape failed at {lname} with in_shape={cur}: {e}"
                if strict:
                    raise RuntimeError(msg) from e
                errors.append(msg)
                cur = None
                break

        self.input_shape = tuple(map(int, input_shape))
        self.output_shape = cur if isinstance(cur, tuple) else None
        self.built = (len(errors) == 0) and (self.output_shape is not None)

        if verify_output and not self.built:
            detail = "\n".join(errors) if errors else "unknown error"
            raise RuntimeError(
                f"[Sequential.build] build incomplete. output_shape={self.output_shape}, "
                f"errors:\n{detail}"
            )

    # -------------------------------------------------------------------------
    # Eager ì‹¤í–‰ (ì°¸ê³ /ë””ë²„ê¹…/í…ŒìŠ¤íŠ¸ìš©)
    # -------------------------------------------------------------------------
    def call(self, x: Any):
        """ì¦‰ì‹œ ì‹¤í–‰ forward (ìº¡ì²˜ ì—†ì´). ë””ë²„ê¹…/í…ŒìŠ¤íŠ¸ìš© + None ê°€ë“œ."""
        out = x
        for i, lyr in enumerate(self.layers):
            if hasattr(lyr, "training"):
                lyr.training = self.training
            out = lyr(out)
            if out is None:
                lname = f"{type(lyr).__name__}:{i}"
                raise RuntimeError(
                    f"[Sequential.call] layer '{lname}' returned None in forward. "
                    f"Check its call() implementation to ensure it returns a tensor."
                )
        return out

    def backward(self, grad_output: Any):
        """ì¦‰ì‹œ ì‹¤í–‰ backward (ìº¡ì²˜ ì—†ì´)."""
        g = grad_output
        for lyr in reversed(self.layers):
            g = lyr.backward(g)
        return g

    def compute_output_shape(self, input_shape: Tuple[int, ...]) -> Tuple[int, ...]:
        """ë ˆì´ì–´ë“¤ì˜ compute_output_shape()ë¥¼ ìˆœì°¨ í˜¸ì¶œí•´ ìµœì¢… ì¶œë ¥ì„ ê³„ì‚°."""
        cur = tuple(map(int, input_shape))
        for lyr in self.layers:
            cur = lyr.compute_output_shape(cur)
        return cur

    def summary(self, indent: int = 2) -> str:
        """ê°„ë‹¨í•œ ìš”ì•½ ë¬¸ìì—´ ìƒì„± (shape/íŒŒë¼ë¯¸í„° ìˆ˜ ë“±)."""
        lines = []
        pad = " " * indent
        lines.append(f"Sequential(name={self.name})")
        if self.input_shape:
            lines.append(f"{pad}Input:  {self.input_shape}")
        cur = self.input_shape
        total_params = 0
        for i, lyr in enumerate(self.layers):
            cls = lyr.__class__.__name__
            shp = None
            if cur is not None:
                try:
                    shp = lyr.compute_output_shape(cur)
                    cur = shp
                except Exception:
                    shp = "?"
                    cur = None
            pcount = 0
            try:
                for (p, _, _) in lyr.parameters():  # type: ignore
                    try:
                        pcount += int(p.size) if hasattr(p, "size") else int(p.size())
                    except Exception:
                        pass
            except Exception:
                pass
            total_params += pcount
            lines.append(f"{pad}[{i:02d}] {cls:>20} -> {shp}  (params={pcount})")
        if cur is not None:
            lines.append(f"{pad}Output: {cur}")
        lines.append(f"{pad}Total params: {total_params}")
        return "\n".join(lines)

    # -------------------------------------------------------------------------
    # í•™ìŠµ ìœ í‹¸
    # -------------------------------------------------------------------------
    def train(self, mode: bool = True):
        """train/eval ëª¨ë“œ í”Œë˜ê·¸ë¥¼ í•˜ìœ„ ë ˆì´ì–´ì— ì „íŒŒ."""
        self.training = bool(mode)
        for lyr in self.layers:
            if hasattr(lyr, "training"):
                lyr.training = self.training
        return self

    def eval(self):
        """eval ëª¨ë“œ ì§„ì… (train(False))"""
        return self.train(False)

    def parameters(self) -> Iterable[Tuple[Any, Any, str]]:
        """(param, grad, tag)ë¥¼ ìˆœíšŒí•˜ë©° ë°©ì¶œ.

        ìš°ì„ ìˆœìœ„:
          1) ë ˆì´ì–´ê°€ parameters()ë¥¼ ì œê³µí•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
          2) í›„ë³´ ì†ì„±ëª… ìŒ(CANDIDATE_PARAM_GRAD_NAMES)
          3) â˜… ì¼ë°˜ íƒìƒ‰: íŒŒë¼ë¯¸í„° ê°ì²´ì˜ `.grad` ì¡´ì¬ ì—¬ë¶€ë¡œ ìˆ˜ì§‘
        """
        for idx, lyr in enumerate(self.layers):
            lname = f"{lyr.__class__.__name__}:{idx}"

            # 1) ë ˆì´ì–´ ìì²´ ì œê³µ
            if hasattr(lyr, "parameters") and callable(getattr(lyr, "parameters")):
                for t in lyr.parameters():  # type: ignore
                    if isinstance(t, tuple) and len(t) == 3:
                        yield t
                    elif isinstance(t, tuple) and len(t) == 2:
                        p, g = t
                        yield (p, g, lname)
                continue

            # 2) ì´ë¦„ ìŒ ë• íƒ€ì´í•‘
            found_named = False
            for p_name, g_name in CANDIDATE_PARAM_GRAD_NAMES:
                if hasattr(lyr, p_name) and hasattr(lyr, g_name):
                    p = getattr(lyr, p_name)
                    g = getattr(lyr, g_name)
                    yield (p, g, f"{lname}.{p_name}")
                    found_named = True
            if found_named:
                continue

            # 3) â˜… ì¼ë°˜ íƒìƒ‰: ìì£¼ ì“°ëŠ” íŒŒë¼ë¯¸í„° ì´ë¦„ì—ì„œ .grad ë¶™ì€ ê°ì²´ ìˆ˜ì§‘
            generic_names = ("W", "weight", "kernel", "b", "bias", "gamma", "beta")
            for p_name in generic_names:
                if hasattr(lyr, p_name):
                    p = getattr(lyr, p_name)
                    g = getattr(p, "grad", None)
                    if g is not None:
                        yield (p, g, f"{lname}.{p_name}")

    def zero_grad(self):
        """ëª¨ë“  íŒŒë¼ë¯¸í„° ê·¸ë˜ë“œë¥¼ 0ìœ¼ë¡œ ì„¤ì •(ê°€ëŠ¥í•˜ë©´ in-place)."""
        for (p, _, _) in self.parameters():
            g = getattr(p, "grad", None)
            if g is not None:
                try:
                    g[...] = 0
                except Exception:
                    try:
                        if hasattr(g, "zero_"):
                            g.zero_()
                        else:
                            setattr(p, "grad", None)
                    except Exception:
                        pass
        for lyr in self.layers:
            if hasattr(lyr, "zero_grad") and callable(getattr(lyr, "zero_grad")):
                try:
                    lyr.zero_grad()  # type: ignore
                except Exception:
                    pass
                continue
            for _, g_name in CANDIDATE_PARAM_GRAD_NAMES:
                if hasattr(lyr, g_name):
                    g = getattr(lyr, g_name)
                    try:
                        g[...] = 0
                    except Exception:
                        try:
                            if hasattr(g, "zero_"):
                                g.zero_()
                            else:
                                setattr(lyr, g_name, None)
                        except Exception:
                            pass

    def attach_grads(self):
        """(p, g) ìŒì´ ì œê³µë˜ëŠ” ê²½ìš° p.gradì— gë¥¼ ì—°ê²°(ì—­í˜¸í™˜)."""
        for (p, g, _) in self.parameters():
            if g is not None:
                try:
                    setattr(p, "grad", g)
                except Exception:
                    pass

    # =========================
    # ===== Graph Capture =====
    # =========================
    def supports_capture(self) -> bool:
        """ëª¨ë“  ë ˆì´ì–´ê°€ capture-safe ì¸í„°í˜ì´ìŠ¤(forward_into/backward_into)ë¥¼ ì§€ì›í•˜ëŠ”ê°€?"""
        ok = True
        for lyr in self.layers:
            f_ok = hasattr(lyr, "forward_into") and callable(getattr(lyr, "forward_into"))
            b_ok = hasattr(lyr, "backward_into") and callable(getattr(lyr, "backward_into"))
            ok = ok and f_ok and b_ok
        return ok

    def compile(
        self,
        input_shape: Tuple[int, ...],
        *,
        loss,
        optimizer,
        lt_bytes: int = (8 << 20),
        stream: Optional[cp.cuda.Stream] = None,
    ) -> "TrainGraph":
        """ì •ì (Graph) ê²½ë¡œ: ì „ì²´ ëª¨ë¸ 1-stepì„ CUDA Graphë¡œ ìº¡ì²˜í•´ ì¬ìƒ ì¤€ë¹„."""
        in_shape = tuple(map(int, input_shape))
        if not self.built:
            self.build(in_shape)

        assert self.supports_capture(), "All layers must implement forward_into/backward_into for capture"

        if stream is None:
            stream = cp.cuda.Stream(non_blocking=True)

        # ==== Pattern pass (no-op) ====
        layers_opt = run_patterns(self.layers)
        model_for_plan = self if layers_opt is self.layers else _ModelLayersProxy(self, layers_opt)

        # 3) ì „ì²´ ëª¨ë¸ìš© ìº¡ì²˜ í”Œëœ ìƒì„±
        plan = make_plan_for_sequential(
            model_for_plan, in_shape, loss_kind="softmax_ce", lt_bytes=lt_bytes
        )

        # 4) ì˜µí‹°ë§ˆì´ì €-ê·¸ë˜ë“œ ë²„í¼ ë¦¬ë°”ì¸ë“œ (ìº¡ì²˜ ì „ ì¼ê´€í™”)
        try_rebind_grads(model_for_plan, optimizer, plan)

        # 5) ìº¡ì²˜-ì„¸ì´í”„ I/O ë²„í¼ (ì»¤ë„ ì œì•½ ê³ ë ¤í•´ fp32/labels=int32)
        X_buf = cp.zeros(in_shape, dtype=cp.float32)
        N = int(in_shape[0])
        y_buf = cp.zeros((N,), dtype=cp.int32)
        loss_buf = cp.zeros((), dtype=cp.float32)

        # 6) CUDA Graph ìº¡ì²˜
        with nvtx_range("[CAPTURE][static]"):
            gexec = record_step_graph(
                model_for_plan,
                loss,
                optimizer.step_into,
                plan,
                X_buf=X_buf,
                y_buf=y_buf,
                stream=stream,
                loss_out=loss_buf,
            )

        io = {"X": X_buf, "y": y_buf, "logits": plan.per_layer[-1].y}

        # â¬‡ï¸ ë¬¸ì„œ/í…ŒìŠ¤íŠ¸ìš© íƒœê·¸ ì „ë‹¬(í‚¤ëŠ” ì •ì  ê²½ë¡œì—ì„  None ìœ ì§€)
        tags = {"nvtx_capture_tag": "[CAPTURE][static]", "nvtx_replay_tag": "[REPLAY]"}
        tg = TrainGraph(gexec, io, stream, plan=plan, graph_key=None, tags=tags)

        # 7) ë‚´ë¶€ í•¸ë“¤ ë³´ê´€
        self._tg = tg
        self._loss_buf = loss_buf
        self._stream = stream
        return tg

    def one_step(self, X, y) -> float:
        """ì •ì (Graph) ê²½ë¡œì˜ 1 step ì¬ìƒ(replay)."""
        assert self._tg is not None, "call compile() first"
        assert self._loss_buf is not None, "loss buffer not initialized"

        xb, yb = self._tg.X_buf, self._tg.y_buf
        x_arr = cp.asarray(X)
        y_arr = cp.asarray(y)

        # ì…ì¶œë ¥ ê°€ë“œ (ì •ì  ê·¸ë˜í”„ëŠ” shape/dtype ë¶ˆë³€ì´ ì›ì¹™)
        assert tuple(xb.shape) == tuple(x_arr.shape), f"X shape mismatch: {x_arr.shape} vs {xb.shape}"
        assert yb.shape == (xb.shape[0],), f"y shape must be (N,), got {yb.shape} vs N={xb.shape[0]}"
        assert yb.dtype == cp.int32, f"labels must be int32 for current CE kernel (got {yb.dtype})"

        self._tg.set_batch(x_arr, y_arr)
        with nvtx_range("[REPLAY][static]"):
            self._tg.launch()
        
        self._tm["replay_count"] += 1  # === NEW ===
        return float(self._loss_buf.get())

    @property
    def tg(self) -> TrainGraph:
        """í˜„ì¬ í™œì„± TrainGraph í•¸ë“¤(ì •ì  ë˜ëŠ” ìµœê·¼ ë™ì  ê²½ë¡œ)ì„ ë°˜í™˜."""
        assert self._tg is not None, "call compile() first"
        return self._tg

    # =========================================================
    # ========== Dynamic Path Handling (ë¶„ê¸°/ë°˜ë³µ) ============
    # =========================================================

    def _infer_signature(self, X, ctx: Dict[str, Any]) -> "GraphSignature":
        """GraphSignature ìƒì„± (shape/dtype/layout ë“± ìµœì†Œ ì •ë³´)."""
        from typing import Any as _AnyType
        if GraphSignature is _AnyType:  # type: ignore[comparison-overlap]
            class _Sig:
                __slots__ = ("shape", "dtype", "layout")
                def __init__(self, shape, dtype, layout):
                    self.shape = tuple(map(int, shape))
                    self.dtype = str(dtype)
                    self.layout = str(layout)
            dtype = getattr(X, "dtype", None)
            dtype_str = str(dtype) if dtype is not None else "fp32"
            shape = tuple(getattr(X, "shape", ()))
            layout = ctx.get("layout", "rowmajor")
            return _Sig(shape, dtype_str, layout)  # type: ignore[return-value]
        dtype = getattr(X, "dtype", None)
        dtype_str = str(dtype) if dtype is not None else "fp32"
        shape = tuple(getattr(X, "shape", ()))
        layout = ctx.get("layout", "rowmajor")
        return GraphSignature(shape=shape, dtype=dtype_str, layout=layout)  # type: ignore[call-arg]

    def _pool_get(self, key: Any) -> Optional[Any]:
        """ê·¸ë˜í”„ í’€(ìˆìœ¼ë©´) ë˜ëŠ” ë¡œì»¬ í´ë°±ì—ì„œ ì—”íŠ¸ë¦¬ ì¡°íšŒ."""
        if graph_pool is not None and hasattr(graph_pool, "get"):
            try:
                entry = graph_pool.get(key)  # global poolì—ì„œ hit/miss ìì²´ ì¹´ìš´íŠ¸
                if entry is not None:
                    self._tm["pool_hit"] += 1
                else:
                    self._tm["pool_miss"] += 1
                return entry
            except Exception:
                pass
        entry = _FALLBACK_POOL.get(key)
        if entry is not None:
            entry["last_used"] = time.monotonic()
            self._tm["pool_hit"] += 1
        else:
            self._tm["pool_miss"] += 1
        return entry

    def _pool_put(self, key: Any, entry: Any) -> None:
        """ê·¸ë˜í”„ í’€(ìˆìœ¼ë©´) ë˜ëŠ” ë¡œì»¬ í´ë°±ì— ì—”íŠ¸ë¦¬ ì €ì¥ (LRU ìƒí•œ ê´€ë¦¬)."""
        if graph_pool is not None and hasattr(graph_pool, "put"):
            try:
                graph_pool.put(key, entry)
                self._tm["pool_put"] += 1
                return
            except Exception:
                pass
        # Fallback with LRU cap
        entry["last_used"] = time.monotonic()
        _FALLBACK_POOL[key] = entry
        self._tm["pool_put"] += 1
        if len(_FALLBACK_POOL) > self._FALLBACK_POOL_MAX:
            # evict LRU
            victim = min(_FALLBACK_POOL.items(), key=lambda kv: kv[1].get("last_used", 0.0))[0]
            _FALLBACK_POOL.pop(victim, None)
            self._tm["pool_evict_fallback"] += 1

    def _make_pool_key(self, sig: Any, ctx: Dict[str, Any], *, loss) -> Any:
        """GraphPool í‚¤ ìƒì„±."""
        branch_path = ctx.get("branch_path")
        if branch_path:
            branch_id = "->".join(map(str, branch_path))
        else:
            branch_id = ctx.get("branch", "default")

        # === NEW === variant êµ¬ì„± ë³´ì • (amp ë¬¸ìì—´ ë°˜ì˜ / variant ìš°ì„ )
        vdict = dict(ctx.get("variant", {}))  # variant ìš°ì„ 
        if "amp" not in vdict and "amp" in ctx:
            vdict["amp"] = ctx.get("amp")
        vdict.setdefault("amp", "fp32")  # ê¸°ë³¸ê°’

        vdict["path_fp"] = tuple(ctx.get("path_fingerprint", ()))
        vdict["training"] = bool(self.training)
        vdict["dtype"] = str(getattr(sig, "dtype", "fp32"))
        vdict["loss_kind"] = getattr(loss, "name", "softmax_ce")

        variant = tuple(sorted((str(k), self._freeze_value(v)) for k, v in vdict.items()))
        try:
            if GraphKey not in (None, object):  # ì•½ì‹ ê°€ë“œ
                return GraphKey(signature=sig, branch_id=str(branch_id), variant=variant)  # type: ignore[call-arg]
        except Exception:
            pass
        return ("dyn",
                tuple(getattr(sig, "shape", ()) ),
                str(getattr(sig, "dtype", "")),
                str(getattr(sig, "layout", "")),
                str(branch_id),
                variant)

    @staticmethod
    def _freeze_value(v: Any) -> Any:
        """ë³€í˜• ê°€ëŠ¥í•œ ê°’ë“¤ì„ í•´ì‹œê°€ëŠ¥í•œ ë¶ˆë³€ ê°’ìœ¼ë¡œ ê³ ì •."""
        if isinstance(v, (str, int, float, bool, type(None))):
            return v
        if isinstance(v, (tuple, list)):
            return tuple(Sequential._freeze_value(x) for x in v)
        if isinstance(v, dict):
            return tuple(sorted((str(k), Sequential._freeze_value(val)) for k, val in v.items()))
        return str(v)

    def _linearize_path(self, X, ctx: Dict[str, Any]) -> List[Layer]:
        """ë™ì  ì œì–´ ë ˆì´ì–´(If/Repeat/EarlyExit)ë¥¼ 'ì‹¤í–‰ëœ ê²½ë¡œ'ë¡œ í‰íƒ„í™”."""
        def _is_if(obj):
            return callable(getattr(obj, "decide", None)) and \
                hasattr(obj, "then_block") and hasattr(obj, "else_block")

        def _is_repeat(obj):
            return callable(getattr(obj, "steps", None)) and hasattr(obj, "body")

        def _is_early(obj):
            return hasattr(obj, "stages") and isinstance(getattr(obj, "stages"), (list, tuple))

        # ëˆ„ì  ë¶„ê¸° ê²½ë¡œ ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™”
        if "branch_path" not in ctx:
            ctx["branch_path"] = tuple()

        linear: List[Layer] = []
        for l in self.layers:
            if _is_if(l):
                branch, block = l.decide(X, ctx)
                ctx["branch_path"] = tuple(ctx["branch_path"]) + (branch,)
                ctx["branch"] = branch  # ë‹¨ì¼ í‚¤ë„ ìœ ì§€ (ë ˆê±°ì‹œ)
                if isinstance(block, Sequential):
                    linear.extend(block.layers)
                else:
                    linear.append(block)

            elif _is_repeat(l):
                T = int(l.steps(X, ctx))
                ctx["repeat_steps"] = T
                body = l.body
                if isinstance(body, Sequential):
                    linear.extend(body.layers)
                else:
                    linear.append(body)

            elif _is_early(l):
                stages = list(l.stages)
                for k, s in enumerate(stages):
                    if isinstance(s, Sequential):
                        linear.extend(s.layers)
                    else:
                        linear.append(s)
                    if callable(getattr(l, "exit_fn", None)) and l.exit_fn(ctx):
                        ctx["branch_path"] = tuple(ctx["branch_path"]) + (f"ee:{k}",)
                        break
                ctx["earlyexit"] = True

            else:
                linear.append(l)

        # âœ… ì»¨íŠ¸ë¡¤ ë ˆì´ì–´ ì”ì¡´ ê°€ë“œ (í‰íƒ„í™” ëˆ„ë½ ë°©ì§€)
        leftovers = []
        def _is_ctrl(x):
            return _is_if(x) or _is_repeat(x) or _is_early(x)
        for x in linear:
            if _is_ctrl(x):
                leftovers.append(type(x).__name__)
        if leftovers:
            raise RuntimeError(
                f"[dynamic] control layers must be flattened, but found in path: {leftovers}"
            )

        # ê²½ë¡œ fingerprint ì €ì¥ (ë ˆì´ì–´ í´ë˜ìŠ¤ ì‹œí€€ìŠ¤)
        ctx["path_fingerprint"] = tuple(type(l).__name__ for l in linear)
        return linear

    @staticmethod
    def _ensure_path_captureable(layers: Sequence[Layer]) -> None:
        """ê²½ë¡œ ë‚´ ëª¨ë“  ë ˆì´ì–´ê°€ capture-safe ì¸í„°í˜ì´ìŠ¤ë¥¼ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸."""
        for lyr in layers:
            if not (hasattr(lyr, "forward_into") and hasattr(lyr, "backward_into")):
                raise AssertionError(f"Layer {type(lyr).__name__} not capture-ready")

    def _get_or_capture_dynamic_entry(
        self,
        X: cp.ndarray,
        y: cp.ndarray,
        *,
        loss,
        optimizer,
        ctx: Dict[str, Any],
        lt_bytes: int,
        stream: Optional[cp.cuda.Stream],
    ) -> Dict[str, Any]:
        """ë™ì  ê²½ë¡œì˜ í•µì‹¬ ì§„ì…ì : í‰íƒ„í™”â†’íŒ¨í„´â†’í‚¤â†’ìºì‹œâ†’(ë¯¸ìŠ¤)ìº¡ì²˜."""
        # 1) ê²½ë¡œ í‰íƒ„í™”
        with nvtx_range("[DYN] path_linearize"):
            path_layers = self._linearize_path(X, ctx)
        self._ensure_path_captureable(path_layers)

        # ==== Pattern pass (no-op) ====
        with nvtx_range("[DYN] patterns"):
            path_layers = rewrite(path_layers)

        # 2) í‚¤ êµ¬ì„± (GraphSignature + branch_path ë“±)
        with nvtx_range("[DYN] make_pool_key"):
            sig = self._infer_signature(X, ctx)
            key = self._make_pool_key(sig, ctx, loss=loss)

        # 3) í’€ ì¡°íšŒ (GraphPool â†’ Fallback dict)
        with nvtx_range("[DYN] get_from_pool"):
            entry = self._pool_get(key)
            if entry is not None:
                return entry

        # 4) ì‹ ê·œ ìº¡ì²˜ (ë¯¸ìŠ¤ ì‹œ)
        in_shape = tuple(map(int, getattr(sig, "shape", tuple(X.shape))))

        # ê²½ë¡œ ë ˆì´ì–´ ì¬ë¹Œë“œ(ë°°ì¹˜/íƒ€ì„ ë³€í™” ëŒ€ì‘)
        def _rebuild_path_layers(layers, ish):
            cur = tuple(ish)
            for lyr in layers:
                try:
                    if hasattr(lyr, "build"):
                        lyr.build(cur)
                except Exception:
                    pass
                try:
                    if hasattr(lyr, "compute_output_shape"):
                        cur = tuple(map(int, lyr.compute_output_shape(cur)))
                except Exception:
                    pass

        if not self.built:
            self.build(in_shape)
        else:
            if tuple(self.input_shape or ()) != in_shape:
                _rebuild_path_layers(path_layers, in_shape)

        if stream is None:
            stream = cp.cuda.Stream(non_blocking=True)

        # ë™ì  ê²½ë¡œ ì „ìš© í”Œëœ
        with nvtx_range("[DYN] make_capture_plan"):
            plan = make_plan_for_path(
                path_layers, in_shape, loss_kind=getattr(loss, "name", "softmax_ce"), lt_bytes=lt_bytes
            )

        # ---- ê²½ë¡œ ì „ìš© (param, grad) íŠ¸ë¦¬í”Œ ìˆ˜ì§‘: ì •í™• ë§¤í•‘ + ì¤‘ë³µ ë°©ì§€ ----
        def _collect_triplets_from_path(plan, layers):
            triplets = []
            seen = set()
            def push(p, g, tag):
                key2 = (
                    int(getattr(getattr(p, "data", p), "ptr", id(p))),
                    int(getattr(getattr(g, "data", g), "ptr", id(g)))
                )
                if key2 not in seen:
                    triplets.append((p, g, tag))
                    seen.add(key2)

            for i, lyr in enumerate(layers):
                per = plan.per_layer[i]
                # Dense/Conv ê³µí†µ
                if hasattr(lyr, "W") and per.gW is not None:
                    push(getattr(lyr, "W"), per.gW, f"{type(lyr).__name__}:{i}.W")
                for b_name in ("b", "bias", "B"):
                    if hasattr(lyr, b_name) and getattr(lyr, b_name) is not None and per.gB is not None:
                        push(getattr(lyr, b_name), per.gB, f"{type(lyr).__name__}:{i}.{b_name}")
                        break
                # BN
                if hasattr(lyr, "gamma") and per.gW is not None:
                    try:
                        if tuple(getattr(lyr, "gamma").shape) == tuple(per.gW.shape):
                            push(getattr(lyr, "gamma"), per.gW, f"BN2d:{i}.gamma")
                    except Exception:
                        pass
                if hasattr(lyr, "beta") and per.gB is not None:
                    try:
                        if tuple(getattr(lyr, "beta").shape) == tuple(per.gB.shape):
                            push(getattr(lyr, "beta"), per.gB, f"BN2d:{i}.beta")
                    except Exception:
                        pass
                # RNN
                for w_name, g_name, tag in (("Wx", "gWx", "Wx"), ("Wh", "gWh", "Wh")):
                    if hasattr(lyr, w_name) and getattr(per, g_name, None) is not None:
                        push(getattr(lyr, w_name), getattr(per, g_name), f"RNN:{i}.{tag}")
                if hasattr(lyr, "b") and getattr(per, "gB", None) is not None and getattr(lyr, "b") is not None:
                    push(getattr(lyr, "b"), per.gB, f"RNN:{i}.b")
            return triplets

        triplets = _collect_triplets_from_path(plan, path_layers)

        # ---- ì˜µí‹°ë§ˆì´ì € ë°”ì¸ë”© (ê²½ë¡œë³„ ì˜µí‹°ë§ˆì´ì € ìƒì„±/ìºì‹œ or ì¬ë°”ì¸ë“œ) ----
        opt_for_path = optimizer

        def _new_opt_like(base_opt):
            OptCls = base_opt.__class__
            hyper = {}
            for k in ("lr", "wd", "weight_decay", "beta1", "beta2", "betas", "eps"):
                if hasattr(base_opt, k):
                    hyper[k] = getattr(base_opt, k)
            try:
                return OptCls([], **hyper)
            except TypeError:
                return OptCls([])

        with nvtx_range("[DYN] rebind"):
            try:
                if hasattr(opt_for_path, "rebind_grads"):
                    opt_for_path.rebind_grads(triplets)
                else:
                    raise AssertionError("optimizer has no rebind_grads")
            except AssertionError:
                opt_for_path = _new_opt_like(optimizer)
                if hasattr(opt_for_path, "ensure_initialized"):
                    try:
                        opt_for_path.ensure_initialized()
                    except Exception:
                        pass
                opt_for_path.rebind_grads(triplets)

        # 5) ê³ ì • I/O ë²„í¼ (í˜„ì¬ ì»¤ë„ ì œì•½ìƒ fp32/int32ê°€ ì•ˆì „)
        X_buf = cp.zeros(in_shape, dtype=cp.float32)
        N = int(in_shape[0])
        y_buf = cp.zeros((N,), dtype=cp.int32)
        loss_buf = cp.zeros((), dtype=cp.float32)

        # ---- record_step_graph í•˜ìœ„í˜¸í™˜ ì²˜ë¦¬ ----
        try:
            sig_rs = inspect.signature(record_step_graph)
            has_layers_override = ("layers_override" in sig_rs.parameters)
        except Exception:
            has_layers_override = False

        with nvtx_range(f"[DYN] record_step_graph path={ctx.get('path_fingerprint')}"):
            if has_layers_override:
                gexec = record_step_graph(
                    self,
                    loss,
                    opt_for_path.step_into,
                    plan,
                    X_buf=X_buf,
                    y_buf=y_buf,
                    stream=stream,
                    loss_out=loss_buf,
                    layers_override=path_layers,
                    # graph_keyëŠ” TrainGraphë¡œ ì „ë‹¬ë§Œ í•˜ê³  record_step_graph ë‚´ë¶€ì—ì„  ì‚¬ìš©í•˜ì§€ ì•Šì•„ë„ OK
                    graph_key=key,
                )
            else:
                # layers_override ë¯¸ì§€ì› record_step_graphì— ëŒ€í•œ í˜¸í™˜
                proxy_model = _ModelLayersProxy(self, path_layers)
                gexec = record_step_graph(
                    proxy_model,
                    loss,
                    opt_for_path.step_into,
                    plan,
                    X_buf=X_buf,
                    y_buf=y_buf,
                    stream=stream,
                    loss_out=loss_buf,
                    graph_key=key,
                )

        io = {"X": X_buf, "y": y_buf, "logits": plan.per_layer[-1].y}

        # â¬‡ï¸ ë¬¸ì„œ/í…ŒìŠ¤íŠ¸ìš© íƒœê·¸ ì „ë‹¬
        tags = {
            "nvtx_capture_tag": "[DYN][CAPTURE]",
            "nvtx_replay_tag": "[DYN][REPLAY]",
            "path_fingerprint": tuple(ctx.get("path_fingerprint", ())),
            "branch_path": tuple(ctx.get("branch_path", ())),
        }
        tg = TrainGraph(gexec, io, stream, plan=plan, graph_key=key, tags=tags)

        entry = {
            "tg": tg,
            "loss_buf": loss_buf,
            "stream": stream,
            "optimizer": opt_for_path,
            "plan": plan,  # Dropout counter advance ë“±ì— ì‚¬ìš©
        }
        self._tm["capture_count"] += 1  # === NEW ===
        self._pool_put(key, entry)
        return entry

    def one_step_dynamic(
        self,
        X,
        y,
        *,
        loss,
        optimizer,
        ctx: Optional[Dict[str, Any]] = None,
        lt_bytes: int = (8 << 20),
        stream: Optional[cp.cuda.Stream] = None,
    ) -> float:
        """ë™ì (Graph) ê²½ë¡œ: If/Repeat/EarlyExit í¬í•¨í•œ 'í˜„ì¬ ì‹¤í–‰ëœ ê²½ë¡œ'ë¥¼ ìº¡ì²˜/ì¬ìƒ."""
        ctx = dict(ctx or {})
        x_arr = cp.asarray(X)
        y_arr = cp.asarray(y)

        entry = self._get_or_capture_dynamic_entry(
            x_arr, y_arr, loss=loss, optimizer=optimizer,
            ctx=ctx, lt_bytes=lt_bytes, stream=stream
        )

        tg: TrainGraph = entry["tg"]
        loss_buf: cp.ndarray = entry["loss_buf"]
        plan = entry.get("plan", None)

        # âœ… í˜„ì¬ ë™ì  ê²½ë¡œ ê·¸ë˜í”„ í•¸ë“¤ì„ ëª¨ë¸ ìˆ˜ì¤€ í•¸ë“¤ë¡œ ë…¸ì¶œ (ì™¸ë¶€ ì‚¬ìš© ìš©ì´)
        self._tg = tg
        self._loss_buf = loss_buf
        self._stream = entry.get("stream", self._stream)

        # ëª¨ì–‘/íƒ€ì… ê°€ë“œ
        assert tuple(tg.X_buf.shape) == tuple(x_arr.shape), \
            f"[dynamic] X shape mismatch: {x_arr.shape} vs {tg.X_buf.shape}"
        assert tg.y_buf.shape == (tg.X_buf.shape[0],), \
            f"[dynamic] y shape must be (N,), got {tg.y_buf.shape} vs N={tg.X_buf.shape[0]}"
        assert tg.y_buf.dtype == cp.int32, \
            f"[dynamic] labels must be int32 (got {tg.y_buf.dtype})"

        # ê³ ì • ë²„í¼ì— ë°°ì¹˜ ë³µì‚¬
        tg.set_batch(x_arr, y_arr)

        # Repeat: ìº¡ì²˜ëŠ” 1 step ê¸°ì¤€, ì‹¤í–‰ ì‹œ TíšŒ launch
        T = int(ctx.get("repeat_steps", 1))
        rep_batches = ctx.get("repeat_batches", None)  # [(X_t, y_t), ...] ê°€ëŠ¥

        with nvtx_range(f"[DYN] replay path={ctx.get('path_fingerprint')} x{T}"):
            if isinstance(rep_batches, (list, tuple)) and len(rep_batches) >= T:
                for t in range(T):
                    if plan is not None:
                        advance_dropout(plan, seed_bump=t)
                    xb_t = cp.asarray(rep_batches[t][0])
                    yb_t = cp.asarray(rep_batches[t][1])
                    assert tuple(tg.X_buf.shape) == tuple(xb_t.shape), "[dynamic] repeat batch X shape mismatch"
                    assert tg.y_buf.shape == (tg.X_buf.shape[0],), "[dynamic] repeat batch y shape mismatch"
                    tg.set_batch(xb_t, yb_t)
                    tg.launch()
            else:
                for t in range(max(1, T)):
                    if plan is not None:
                        advance_dropout(plan, seed_bump=t)
                    tg.launch()

        # ì†ì‹¤ ìŠ¤ì¹¼ë¼ ë°˜í™˜
        self._tm["replay_count"] += max(1, int(ctx.get("repeat_steps", 1)))  # === NEW ===

        return float(loss_buf.get())

    # ======== NEW: Frontend convenience APIs (fit/warmup/replay & pool tools) ========

    def fit(
        self,
        data_loader,
        *,
        loss,
        optimizer,
        ctx: Optional[Dict[str, Any]] = None,
        epochs: int = 1,
        use_dynamic: bool = True,
        static_input_shape: Optional[Tuple[int, ...]] = None,
        prewarm_samples: Optional[Sequence[Tuple[Any, Any, Dict[str, Any]]]] = None,
        report_every: int = 100,
    ):
        """
        ì¼€ë¼ìŠ¤/íŒŒì´í† ì¹˜ ëŠë‚Œì˜ ê³ ìˆ˜ì¤€ í•™ìŠµ ë£¨í”„.
        - use_dynamic=True: one_step_dynamic ê²½ë¡œ ì‚¬ìš©(ë¶„ê¸°/ë°˜ë³µ ì§€ì›, on-demand capture)
        - use_dynamic=False: ì •ì  compile/one_step ì‚¬ìš©(ì…ë ¥ shape ê³ ì • í•„ìš”)
        - prewarm_samples: [(X, y, ctx), ...] í˜•íƒœë¡œ ë¯¸ë¦¬ GraphKeyë¥¼ ìº¡ì²˜í•´ hitìœ¨â†‘
        """
        ctx = dict(ctx or {})

        if not use_dynamic:
            assert static_input_shape is not None, "static_input_shape is required for static fit"
            self.compile(static_input_shape, loss=loss, optimizer=optimizer)

        if prewarm_samples:
            for Xw, yw, cw in prewarm_samples:
                _ = self.one_step_dynamic(Xw, yw, loss=loss, optimizer=optimizer, ctx=cw)

        step = 0
        for ep in range(epochs):
            for X, y in data_loader:
                if use_dynamic:
                    loss_val = self.one_step_dynamic(X, y, loss=loss, optimizer=optimizer, ctx=ctx)
                else:
                    loss_val = self.one_step(X, y)
                step += 1
                if report_every and (step % report_every == 0):
                    print(f"[fit] epoch={ep} step={step} loss={loss_val:.6f}")

    def warmup(
        self,
        samples: Sequence[Tuple[Any, Any, Dict[str, Any]]],
        *,
        loss,
        optimizer,
    ) -> Dict[Tuple[Tuple[str, Any], ...], "TrainGraph"]:
        """
        ì—¬ëŸ¬ (X,y,ctx) ì¡°í•©ìœ¼ë¡œ GraphKeyë¥¼ ë¯¸ë¦¬ ìº¡ì²˜í•´ ë‘ .
        ë°˜í™˜: { variant_kv_tuple: TrainGraph }
        """
        out = {}
        for X, y, ctx in samples:
            _ = self.one_step_dynamic(X, y, loss=loss, optimizer=optimizer, ctx=ctx)
            var = tuple(sorted((str(k), self._freeze_value(v)) for k, v in dict(ctx.get("variant", {})).items()))
            out[var] = self.tg
        return out

    def replay_loop(
        self,
        batches: Iterable[Tuple[Any, Any]],
        *,
        steps: Optional[int] = None,
    ):
        """
        ì´ë¯¸ ìº¡ì²˜ëœ self.tg(TrainGraph)ë¡œ í•«ë£¨í”„ ì‹¤í–‰.
        - set_batch() + launch()ë§Œ ìˆ˜í–‰ â†’ Python ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
        - ì‚¬ì „ì— self.tgê°€ ì¡´ì¬í•´ì•¼ í•¨ (compile() ë˜ëŠ” warmup/one_step_dynamic()ìœ¼ë¡œ ìƒì„±)
        """
        assert self._tg is not None, "No captured graph. Call compile() or warmup/one_step_dynamic() first."
        n = 0
        for X, y in batches:
            self._tg.set_batch(cp.asarray(X), cp.asarray(y))
            self._tg.launch()
            n += 1
            if steps and n >= steps:
                break

    def pool_stats(self) -> Dict[str, Any]:
        """
        í˜„ì¬ ê·¸ë˜í”„ í’€ ìš”ì•½.
        - ê¸€ë¡œë²Œ í’€ ì‚¬ìš© ì‹œ: í¬ê¸°ë§Œ ë…¸ì¶œ(ë‚´ë¶€ êµ¬ì¡° ì€ë‹‰ ê°€ì •)
        - í´ë°± í’€ ì‚¬ìš© ì‹œ: í‚¤ ìˆ˜/ìµœì¢… ì‚¬ìš© ì‹œê° ë“± ìš”ì•½
        """
        stats = {"global": False, "fallback_size": 0, "fallback_cap": self._FALLBACK_POOL_MAX}
        try:
            if graph_pool is not None and hasattr(graph_pool, "_store"):
                stats["global"] = True
                stats["global_size"] = len(getattr(graph_pool, "_store"))
        except Exception:
            pass
        try:
            from time import monotonic
            stats["fallback_size"] = len(_FALLBACK_POOL)
            if _FALLBACK_POOL:
                last_used = [v.get("last_used", 0.0) for v in _FALLBACK_POOL.values()]
                stats["fallback_oldest_sec"] = max(0.0, (monotonic() - min(last_used)))
        except Exception:
            pass

        stats["local_tm"] = dict(self._tm)

        return stats

    def get_graph_key_preview(self, X, *, ctx: Optional[Dict[str, Any]] = None, loss=None):
        """
        ì‹¤ì œ ìº¡ì²˜ ì—†ì´ 'í˜„ì¬ ì…ë ¥+ì»¨í…ìŠ¤íŠ¸'ë¡œ ìƒì„±ë  GraphKeyë¥¼ ë¯¸ë¦¬ ì‚°ì¶œ.
        - ë””ë²„ê·¸/ë¡œê¹…/ë©”íŠ¸ë¦­ì— ìœ ìš©
        """
        ctx = dict(ctx or {})
        sig = self._infer_signature(cp.asarray(X), ctx)
        return self._make_pool_key(sig, ctx, loss=loss)

    def evict_pool(self, *, predicate=None, max_remove: Optional[int] = None):
        """
        í´ë°± LRU í’€ì—ì„œ ì¡°ê±´ë¶€ë¡œ ì—”íŠ¸ë¦¬ë¥¼ ì œê±°. (ê¸€ë¡œë²Œ í’€ì€ ìš´ì˜ ì •ì±…ì— ë”°ë¦„)
        predicate(key, entry) â†’ bool ì´ Trueì¸ í•­ëª©ë§Œ ì œê±°.
        """
        removed = 0
        keys = list(_FALLBACK_POOL.keys())
        for k in keys:
            if predicate is None or predicate(k, _FALLBACK_POOL[k]):
                _FALLBACK_POOL.pop(k, None)
                removed += 1
                if max_remove and removed >= max_remove:
                    break
        return removed
    
    # === NEW ===
    def telemetry(self) -> Dict[str, int]:
        """ë¡œì»¬ Sequential ë‹¨ìœ„ í…”ë ˆë©”íŠ¸ë¦¬ ì¹´ìš´í„° ë°˜í™˜."""
        return dict(self._tm)

    # === NEW ===
    def reset_telemetry(self) -> None:
        """ë¡œì»¬ Sequential í…”ë ˆë©”íŠ¸ë¦¬ ì´ˆê¸°í™”."""
        for k in self._tm.keys():
            self._tm[k] = 0

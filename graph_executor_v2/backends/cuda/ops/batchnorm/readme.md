ğŸ§© Module: backends/cuda/ops/batchnorm

Batch Normalization (BN) operator implemented as an independent CUDA module.

êµ¬ì„±:

ğŸ“¦ backends/cuda/ops/batchnorm
 â”£ ğŸ“œ api.hpp
 â”£ ğŸ“œ launcher.cu
 â”— ğŸ“œ kernels.cu

1ï¸âƒ£ Overview

ì´ ëª¨ë“ˆì€ Batch Normalization (BN) ì˜ forward / backward ì—°ì‚°ì„ CUDA ìƒì—ì„œ ìˆ˜í–‰í•œë‹¤.
ì£¼ìš” íŠ¹ì§•:

NCHW / NHWC ë ˆì´ì•„ì›ƒ ëª¨ë‘ ì§€ì› (attrs.channels_last).

í•™ìŠµ / ì¶”ë¡  ê²½ë¡œ ëª¨ë‘ í¬í•¨.

í˜¼í•© ì •ë°€ë„ ì§€ì› êµ¬ì¡°ë¥¼ ê°€ì •í•˜ì§€ë§Œ, í˜„ì¬ ì»¤ë„ì€ FP32 ì „ìš©.

CUDA Graph ìº¡ì²˜ ì„¸ì´í”„, ë‚´ë¶€ ë™ì  í• ë‹¹ ì—†ìŒ.

1 channel = 1 CTA êµ¬ì¡°ë¡œ ë‹¨ìˆœí•˜ê³  ê²°ì •ì (deterministic) ì—°ì‚°.

ì™¸ë¶€ì—ì„œ ì£¼ì–´ì§€ëŠ” Tensor í¬ì¸í„° ê¸°ë°˜ API, ë‚´ë¶€ì—ì„œ ë©”ëª¨ë¦¬ í• ë‹¹í•˜ì§€ ì•ŠìŒ.

2ï¸âƒ£ íŒŒì¼ êµ¬ì¡°ë³„ ì—­í• 
íŒŒì¼	ì—­í•  ìš”ì•½
api.hpp	ì™¸ë¶€ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê³µì‹ API ì‹œê·¸ë‹ˆì²˜ ë° ì†ì„± ì •ì˜ (BatchNormAttrs, BatchNormCudaLaunch, BatchNormCudaBackwardLaunch)
launcher.cu	shape/type ê²€ì¦, ê° CUDA kernel ì‹¤í–‰ ìˆœì„œ ê´€ë¦¬, í•™ìŠµ/ì¶”ë¡  ë¶„ê¸° ë° EMA(running stats) ì—…ë°ì´íŠ¸ ìˆ˜í–‰
kernels.cu	ì‹¤ì œ CUDA device-level ê³„ì‚° ì»¤ë„ ì •ì˜ (mean/var reduce, normalize+affine, backward gradients)
3ï¸âƒ£ Data Layout / Precision

ì…ë ¥/ì¶œë ¥: FP32 tensor (Tensor.desc.dtype == F32)

ë‚´ë¶€ ëˆ„ì : FP32 (ë¶€ë¶„ í•©, ë¶„ì‚° ê³„ì‚° í¬í•¨)

ë ˆì´ì•„ì›ƒ:

channels_last == false â†’ NCHW ([N,C,H,W])

channels_last == true â†’ NHWC ([N,H,W,C])

í…ì„œ í™•ì¸ ìœ í‹¸:

is4_f32_cuda(t) â†’ 4D FP32 CUDA tensor

is1_f32_cuda(t) â†’ 1D FP32 CUDA tensor (e.g., mean/var/gamma/beta)

4ï¸âƒ£ Forward Path (BatchNormCudaLaunch)
ê²½ë¡œ ì„ íƒ

í•™ìŠµ ëª¨ë“œ (attrs.training = true)

Mean / Var ê³„ì‚°

welford_reduce_meanvar_launcher() í˜¸ì¶œ

ê° ì±„ë„ì— ëŒ€í•´ í•œ CTA(block)ê°€ Î£x, Î£xÂ² ê³„ì‚°

running_var ë²„í¼ë¥¼ ì„ì‹œ var ì €ì¥ìš©ìœ¼ë¡œ ì‚¬ìš©

invstd ê³„ì‚°

compute_invstd_kernel() â†’ invstd = rsqrt(var + eps)

ê²°ê³¼ save_invstdì— ì €ì¥

ì •ê·œí™” + Affine ë³€í™˜

bn_forward_normalize_affine_launcher()

Y = ((X - mean) * invstd) * gamma + beta

Running statistics ì—…ë°ì´íŠ¸ (EMA)

bn_update_running_kernel()

running_mean â† (1-m)*running_mean + m*batch_mean

running_var â† (1-m)*running_var + m*batch_var

batch_varëŠ” running_var ë²„í¼ë¥¼ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©í•¨

**save_mean, save_invstd**ëŠ” backwardë¥¼ ìœ„í•œ ì¶œë ¥

ì¶”ë¡  ëª¨ë“œ (attrs.training = false)

running_varë¡œë¶€í„° invstd ê³„ì‚° (compute_invstd_kernel)

ì •ê·œí™” ë° affine ì ìš© (bn_forward_normalize_affine_launcher)

í˜„ì¬ êµ¬í˜„ì—ì„œëŠ” save_invstd == nullptrì´ë©´ MissingInputìœ¼ë¡œ ì¢…ë£Œ
â†’ ë³„ë„ì˜ ì„ì‹œ ë²„í¼ë¥¼ ë‚´ë¶€ì—ì„œ ìƒì„±í•˜ì§€ ì•ŠìŒ (ìº¡ì²˜ ì„¸ì´í”„ ì„¤ê³„)

5ï¸âƒ£ Backward Path (BatchNormCudaBackwardLaunch)

dgamma, dbeta ê³„ì‚°

bn_backward_reduce_dbeta_dgamma_launcher()

ì±„ë„ë³„ 1 CTAë¡œ ë‹¤ìŒì„ ê³„ì‚°:

dbeta[c] += Î£ dY

dgamma[c] += Î£ dY * xÌ‚ (ë‹¨, with_affine==trueì¼ ë•Œë§Œ)

dbeta, dgammaëŠ” ëŸ°ì²˜ í˜¸ì¶œ ì „ 0ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ì•¼ í•¨ (+= ëˆ„ì )

dX ê³„ì‚°

bn_backward_dx_launcher()

ë‚´ë¶€ ìˆ˜ì‹:

dyÎ³ = dY * (gamma or 1)
S1 = Î£ dyÎ³
S2 = Î£ dyÎ³ * xÌ‚
dX = (1/M) * invstd * (M*dyÎ³ - S1 - xÌ‚*S2)


ê° ì±„ë„ë§ˆë‹¤ CTA 1ê°œë¡œ 2-phase ìˆ˜í–‰ (reduce â†’ write)

6ï¸âƒ£ Kernel êµ¬ì„± (kernels.cu)
ì»¤ë„ ì´ë¦„	ì—­í• 	CTA êµ¬ì¡°	Shared Memory	Precision	ì„¤ëª…
reduce_mean_var_kernel	Xë¡œë¶€í„° mean, var ê³„ì‚°	channelë‹¹ 1 CTA	2Ã—blockDim floats (sum, sumsq)	accum FP64, reduce FP32	biased var (1/M), per-channel reduction
bn_forward_norm_affine_kernel	normalize + affine	channelë‹¹ 1 CTA	ì—†ìŒ	FP32	Y=(X-Î¼)*invstd*Î³+Î²
bn_bwd_dbeta_dgamma_kernel	dbeta/dgamma ê°ì†Œ	channelë‹¹ 1 CTA	2Ã—blockDim floats	FP32	dbeta=Î£dY, dgamma=Î£dY*xÌ‚
bn_bwd_dx_kernel	dX ê³„ì‚°	channelë‹¹ 1 CTA	2Ã—blockDim floats	FP32	(2-phase reduction + write)
bn_update_running_kernel	running stats EMA	grid over C	ì—†ìŒ	FP32	(1-m)*running + m*batch
compute_invstd_kernel	invstd ê³„ì‚°	grid over C	ì—†ìŒ	FP32	invstd = 1/sqrt(var + eps)

ê³µí†µ ì„¤ì •

blockDim = 256

gridDim = C (1 CTA per channel)

ëª¨ë“  ì»¤ë„ì€ cudaGraph capture-safe (ë™ì  ë©”ëª¨ë¦¬ ì—†ìŒ)

deterministic (block ìˆ˜ = C, atomic ì—†ìŒ)

7ï¸âƒ£ Numerical Behavior

Var ê³„ì‚°: biased (1/M), unbiased(1/(M-1)) ì•„ë‹˜

Reduction precision: thread-local FP64, shared FP32

Accumulation order deterministic (1 CTA per channel)

Epsilon: ì ìš© ìœ„ì¹˜ invstd = 1/sqrt(var + eps)

Running stat update: PyTorch-style momentum

running = (1 - m)*running + m*batch

8ï¸âƒ£ Supported / Unsupported
í•­ëª©	ì§€ì› ìƒíƒœ
Layout	NCHW / NHWC
DType	FP32 only
Mixed precision (FP16/BF16)	âš ï¸ êµ¬ì¡°ë§Œ ì¡´ì¬, ë¯¸êµ¬í˜„
Capture-safe execution	âœ…
GroupNorm (num_groups>1)	âŒ (í˜„ì¬ BN only)
Deterministic	âœ… (channelë‹¹ CTA 1ê°œ, atomic ì—†ìŒ)
Gradient wrt running stats	âŒ (í†µê³„ëŠ” EMAë¡œë§Œ ì—…ë°ì´íŠ¸)
Workspace (ws_fwd/ws_bwd)	êµ¬ì¡°ë§Œ ì¡´ì¬, ì‹¤ì œ ì‚¬ìš© ì—†ìŒ
9ï¸âƒ£ Typical Kernel Launch Flow (Training)
# in launcher.cu
--------------------------------------------
welford_reduce_meanvar_launcher()      # mean,var
compute_invstd_kernel()                # invstd = rsqrt(var + eps)
bn_forward_normalize_affine_launcher() # normalize + affine
bn_update_running_kernel()             # EMA update
--------------------------------------------

ğŸ”Ÿ API ê·œì•½ ìš”ì•½ (api.hpp)
í•­ëª©	ì˜ë¯¸
BatchNormAttrs::channels_last	true â†’ NHWC, false â†’ NCHW
eps	ë¶„ì‚° ì•ˆì •í™” Îµ
momentum	running stats EMA ê³„ìˆ˜
training	true: í•™ìŠµ, false: ì¶”ë¡ 
with_affine	gamma/beta ì ìš© ì—¬ë¶€
use_welford	ìˆ˜ì¹˜ ì•ˆì •ì„± ì˜µì…˜ (í˜„ì¬ Î£xÂ² êµ¬í˜„)
BatchNormWorkspaceFwd/Bwd	ì„ íƒì  ë²„í¼ (í˜„ì¬ ì‚¬ìš© ì•ˆ í•¨)
Status::Ok	ì •ìƒ ìˆ˜í–‰
ê¸°íƒ€ Status::*	shape/dtype mismatch, missing input ë“± ì˜¤ë¥˜
11ï¸âƒ£ Known Behaviors / Limitations (í˜„ì¬ êµ¬í˜„ ê¸°ì¤€)
êµ¬ë¶„	ë‚´ìš©
ğŸ”¹ Precision	ë‚´ë¶€ ëˆ„ì  FP32, reduce_mean_var_kernelëŠ” FP64 ëˆ„ì  ì‚¬ìš©
ğŸ”¹ Variance	1/M(biased) ë°©ì‹
ğŸ”¹ NHWC ì„±ëŠ¥	Cê°€ í° ê²½ìš° ë©”ëª¨ë¦¬ ë¹„ì—°ì† ì ‘ê·¼ìœ¼ë¡œ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥
ğŸ”¹ running_var ì¬ì‚¬ìš©	batch varë¥¼ running_var ë²„í¼ì— ê³„ì‚° í›„ EMA updateì—ì„œ ê°™ì€ í¬ì¸í„° ì¬ì‚¬ìš©
ğŸ”¹ invstd ë²„í¼	inference ì‹œ save_invstd ì—†ìœ¼ë©´ MissingInput ì˜¤ë¥˜
ğŸ”¹ Mixed precision	FP16/BF16 ì…ë ¥ ì‹œ API validation ì‹¤íŒ¨ (is4_f32_cuda)
ğŸ”¹ Gradient determinism	deterministic (atomic ì—†ìŒ, CTA=C)
ğŸ”¹ Workspace	êµ¬ì¡°ì²´ ì •ì˜ë§Œ ì¡´ì¬, ì‹¤ì œ ë¯¸ì‚¬ìš©
ğŸ”¹ BN fusion	ë³„ë„ì˜ Conv-BN-Fuse ê¸°ëŠ¥ ì—†ìŒ
12ï¸âƒ£ CUDA Graph Capture Safety

ëª¨ë“  ì»¤ë„ì€ cudaMalloc, cudaFree ë“± ë™ì  í˜¸ì¶œ ì—†ìŒ

shape, attr, ws í¬ê¸°ê°€ ê³ ì •ë˜ë©´ graph-safe

ìº¡ì²˜ëœ ê·¸ë˜í”„ëŠ” ë™ì¼ í…ì„œ shape ì¬ì‚¬ìš© ì‹œ ê·¸ëŒ€ë¡œ ì‹¤í–‰ ê°€ëŠ¥

13ï¸âƒ£ Example Usage (í•™ìŠµ)
ai::Tensor X, Y, gamma, beta, running_mean, running_var, save_mean, save_invstd;
ai::BatchNormAttrs attrs;
attrs.training = true;
attrs.with_affine = true;
attrs.channels_last = false;
attrs.eps = 1e-5f;
attrs.momentum = 0.1f;

ai::Status st = ai::BatchNormCudaLaunch(
  X, &gamma, &beta,
  &running_mean, &running_var,
  Y, attrs,
  stream,
  &save_mean, &save_invstd,
  nullptr
);

14ï¸âƒ£ Example Usage (ì—­ì „íŒŒ)
ai::Tensor dY, X, dX, dgamma, dbeta, save_mean, save_invstd;
ai::BatchNormAttrs attrs;
attrs.with_affine = true;
attrs.channels_last = false;

ai::Status st = ai::BatchNormCudaBackwardLaunch(
  dY, X, &gamma,
  save_mean, save_invstd,
  &dX, &dgamma, &dbeta,
  attrs, stream, nullptr
);

15ï¸âƒ£ ì „ì²´ ë°ì´í„° íë¦„ ìš”ì•½
Forward (Training)
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Input X  â”‚
 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â–¼
  mean,var reduce  â†’ save_mean,var
        â–¼
  invstd = 1/sqrt(var + eps)
        â–¼
  Y = ((X - mean)*invstd)*Î³ + Î²
        â–¼
  update running_mean,var (EMA)
        â–¼
     Output Y

Backward
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   dY, X    â”‚
 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â–¼
  Î£ dY, Î£ dY*xÌ‚  â†’ dbeta, dgamma
        â–¼
  dX = invstd/M * (M*dyÎ³ - S1 - xÌ‚*S2)
        â–¼
     Output dX


âœ… ìš”ì•½

ìƒíƒœ: FP32-only, NCHW/NHWC ì§€ì›, Capture-safe

êµ¬ì¡°: Simple, deterministic, 1-CTA-per-channel

ì£¼ìš” íŠ¹ì§•: External tensor-driven API, no malloc, PyTorch-style momentum

ë¯¸ì§€ì›: FP16/BF16, workspace í™œìš©, GroupNorm, Fusion

ì•ˆì •ì„±: ì˜¤ë¥˜ì²˜ë¦¬/ê²€ì¦ ì² ì €, ì»¤ë„ ìì²´ëŠ” ìº¡ì²˜ì„¸ì´í”„
import sys
import os
import ctypes

# CUDA DLL ë¡œë“œ
ctypes.CDLL(r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6\bin\cudart64_12.dll")

# Pybind11 ë¹Œë“œ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), "build", "lib.win-amd64-cpython-312"))

import cupy as cp
import numpy as np
from graph_executor import OpStruct, Shape, run_graph_cuda, run_graph_backward

cp.cuda.Device(0).use()  # âœ… CUDA ë””ë°”ì´ìŠ¤ ëª…ì‹œ

batch_size = 2

# âœ… ì…ë ¥ ìƒ˜í”Œ 2ê°œ
x = cp.array([[1.0, 2.0],
              [2.0, 3.0]], dtype=cp.float32)
W = cp.array([[1.0, 0.0],
              [0.0, 1.0]], dtype=cp.float32)
b = cp.array([[0.5, -0.5]], dtype=cp.float32)

# âœ… ê³„ì‚° ê·¸ë˜í”„ ì •ì˜
E = [
    OpStruct(0, "x0", "W", "linear"),
    OpStruct(1, "linear", "b", "out"),
    OpStruct(3, "out", "", "act_out"),
]

# âœ… í…ì„œ shape ì •ì˜ (ëª¨ë‘ batch ë‹¨ìœ„ë¡œ ì„¤ì •)
shapes = {
    "x0": Shape(batch_size, 2),
    "W": Shape(2, 2),
    "b": Shape(1, 2),
    "linear": Shape(batch_size, 2),
    "out": Shape(batch_size, 2),
    "act_out": Shape(batch_size, 2),
}

# âœ… ì¤‘ê°„ ê²°ê³¼ ë²„í¼ ë“±ë¡ (CUDA ë‚´ë¶€ ì—°ì‚°ì„ ìœ„í•´ í¬ì¸í„° í•„ìš”)
linear_buf = cp.empty((batch_size, 2), dtype=cp.float32)
out_buf = cp.empty((batch_size, 2), dtype=cp.float32)
act_out_buf = cp.empty((batch_size, 2), dtype=cp.float32)

# âœ… í…ì„œ í¬ì¸í„° ì •ì˜ (ì…ë ¥ + ì¤‘ê°„ ê²°ê³¼)
tensors = {
    "x0": int(x.data.ptr),
    "W": int(W.data.ptr),
    "b": int(b.data.ptr),
    "linear": int(linear_buf.data.ptr),
    "out": int(out_buf.data.ptr),
    "act_out": int(act_out_buf.data.ptr),
}

# âœ… Forward ê²°ê³¼ ì €ì¥ ë²„í¼ (hostë¡œ ë³µì‚¬)
out_host = np.zeros((batch_size, 2), dtype=np.float32)

# âœ… Forward ì‹¤í–‰
run_graph_cuda(E, tensors, shapes, out_host, final_output_id="act_out", batch_size=batch_size)

print("âœ… forward output:")
print(out_host)

# âœ… ì—­ì „íŒŒ ì…ë ¥: grad(act_out)
grad_act_out = cp.ones((batch_size, 2), dtype=cp.float32)
print("ë°ì´í„° í™•ì¸", grad_act_out.data.ptr, grad_act_out)

gradient_ptrs = {
    "act_out": int(grad_act_out.data.ptr)
}

# âœ… Backward ì‹¤í–‰
grad_result = run_graph_backward(
    E=E,
    tensors=tensors,
    shapes=shapes,
    gradients=gradient_ptrs,
    final_output_id="act_out",
    batch_size=batch_size
)

# âœ… ê²°ê³¼ ì¶œë ¥
print("\nâœ… ë°˜í™˜ëœ gradient í¬ì¸í„°:")
for name, ptr in grad_result.items():
    print(f"{name}: {ptr}")

print("\nâœ… ì—­ì „íŒŒ ê²°ê³¼ (gradient ë‚´ìš©):")
    
# ğŸ”¹ ì¶œë ¥ ìš°ì„ ìˆœìœ„: W, b, x0, ê·¸ ì™¸
preferred_order = ["W", "b", "x0", "linear", "out", "act_out"]

for name in preferred_order + [k for k in grad_result.keys() if k not in preferred_order]:
    ptr = grad_result.get(name, 0)
    if ptr == 0:
        print(f"[WARNING] {name} returned null pointer. Skipping.")
        continue

    shape = shapes.get(name)
    if shape is None:
        print(f"[WARNING] Shape not found for {name}. Skipping.")
        continue

    size = shape.rows * shape.cols
    grad_np = cp.ndarray((shape.rows, shape.cols), dtype=cp.float32,
                         memptr=cp.cuda.MemoryPointer(
                             cp.cuda.UnownedMemory(ptr, size * 4, 0), 0))
    print(f"{name}:\n{grad_np.get()}")

import typing
import json
import numpy as np


from dev.graph_engine.graph_compiler import GraphCompiler
from dev.losses import losses_mapping
from dev import optimizers
from dev.backend.backend_ops.losses import losses as cuda_losses
from dev import metrics
from dev.backend.backend_ops.activations import activations as cuda_activations
from dev.layers.dense_mat import DenseMat
from dev.layers.activation_mat import ActivationMat

from ..tests.test_setup import import_cuda_module

matrix_ops = import_cuda_module(
    module_name="operations_matrix_cuda",
    build_dir=r"C:/Users/owner/Desktop/AI_framework-dev/dev/backend/backend_ops/operaters/build/lib.win-amd64-cpython-312"
)


class SequentialMat:
    def __init__(self, layers=None):
        self._layers = []
        self.loss = None
        self.loss_name = None
        self.metric = None
        self.optimizer = None
        if layers:
            for layer in layers:
                self.add(layer)

    def add(self, layer):
        if self._layers:
            prev_output_dim = self._layers[-1].output_dim
            layer.build(prev_output_dim)
        elif hasattr(layer, 'input_dim') and layer.input_dim is not None:
            layer.build(layer.input_dim)
        else:
            raise ValueError("ì²« ë²ˆì§¸ ë ˆì´ì–´ëŠ” input_dimì´ í•„ìš”í•©ë‹ˆë‹¤.")
        self._layers.append(layer)

    def compile(self, optimizer=None, loss=None, p_metrics=None, learning_rate=0.001):
        self.optimizer = optimizers.get(optimizer, learning_rate=learning_rate)
        self.loss = cuda_losses.get(loss)
        self.loss_name = loss
        self.metric = metrics.get(p_metrics)

        # âœ… forward_plan êµ¬ì„± (for fast_forward)
        self.forward_plan = []
        for i, layer in enumerate(self._layers):
            if hasattr(layer, "forward_matrix"):
                plan = layer.forward_matrix()
                self.forward_plan.append(plan)
                print(f"[DEBUG] forward_plan[{i}]:", plan)
            else:
                print(f"[WARN] ë ˆì´ì–´ {layer} ëŠ” forward_matrix() ë¯¸êµ¬í˜„ â†’ forward_plan ìƒëµ")

        # âœ… ê³„ì‚° ê·¸ë˜í”„ IR ìƒì„± (ê¸°ì¡´ compile_model ê¸°ëŠ¥ í†µí•©)
        print(f"\nğŸš€ [SequentialMat] compile_model() from compile()")
        input_dim = self._layers[0].input_dim
        self.graph_compiler = GraphCompiler()
        self.graph_compiler.output_ids = list(range(input_dim))
        self.graph_compiler.node_offset = input_dim

        for i, layer in enumerate(self._layers):
            if hasattr(layer, "build") and layer.input_dim is None:
                layer.build(input_dim)
            input_dim = layer.output_dim
            self.graph_compiler.add_layer(layer)

        self.graph_ir = self.graph_compiler.get_graph()

        print("âœ… [SequentialMat] compile complete.")
        print("   â”œâ”€ Total nodes:", self.graph_ir['TotalNodes'])
        print("   â””â”€ Output node IDs:", self.graph_ir['OutputIDs'])

        # ì„ íƒì ìœ¼ë¡œ: self.graph_matrices = self.graph_compiler.get_matrices()


    def predict(self, x):
        output = x
        for layer in self._layers:
            output = layer.call(output)
        return output

    def build_forward_plan(self, x):
        self.backward_plan = []
        output = x.astype(np.float32)

        for layer in self._layers:
            if isinstance(layer, DenseMat):
                layer_input = output.copy()
                output = layer.call(output)

                self.backward_plan.append({
                    "type": "dense",
                    "input": layer_input,
                    "weight": layer.weights.copy(),
                    "bias": layer.bias.copy(),
                })

            elif isinstance(layer, ActivationMat):
                layer_input = output.copy()
                output = layer.call(output)

                self.backward_plan.append({
                    "type": "activation",
                    "activation": layer.activation_name,
                    "input": layer_input,
                })

        return output


    def compute_loss_and_metrics(self, y_pred, y_true):
        loss_val = self.loss(y_true, y_pred)
        metric_val = self.metric(y_pred, y_true)
        return loss_val, metric_val

    def fast_forward(self, x):
        """
        forward_plan ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±ëœ ì—°ì‚° ê³„íšì„ ë”°ë¼ ìˆœì „íŒŒë¥¼ í•œ ë²ˆì— ìˆ˜í–‰.
        ê° ë ˆì´ì–´ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì§€ ì•Šê³ , plan ì •ë³´ë§Œì„ ì‚¬ìš©.
        """
        output = np.atleast_2d(x).astype(np.float32)

        for i, plan in enumerate(self.forward_plan):
            if plan["type"] == "dense":
                # âœ… Dense ì—°ì‚°
                W = plan["weight"]
                b = plan["bias"]
                try:
                    z = matrix_ops.matrix_multiply(output, W)
                    if isinstance(z, tuple):
                        z = z[0]
                except Exception:
                    z = np.dot(output, W)

                bias_tiled = np.tile(b, (output.shape[0], 1))
                try:
                    z = matrix_ops.matrix_add(z, bias_tiled)
                    if isinstance(z, tuple):
                        z = z[0]
                except Exception:
                    z = z + bias_tiled

                output = z  # ë®ì–´ì“°ê¸°

            elif plan["type"] == "activation":
                # âœ… Activation ì—°ì‚°
                act_name = plan["activation"]
                act_func = {
                    "relu": cuda_activations.relu,
                    "sigmoid": cuda_activations.sigmoid,
                    "tanh": cuda_activations.tanh,
                }.get(act_name)

                if act_func is None:
                    raise NotImplementedError(f"[ERROR] í™œì„±í™” í•¨ìˆ˜ '{act_name}' ì§€ì›ë˜ì§€ ì•ŠìŒ")

                output = act_func(output)

            else:
                raise ValueError(f"[ERROR] ì•Œ ìˆ˜ ì—†ëŠ” ë ˆì´ì–´ íƒ€ì… '{plan['type']}'")

        return output

    def fit(self, x, y, epochs=1, batch_size=-1):
        if batch_size == -1 or batch_size < x.shape[0]:
            batch_size = x.shape[0]
        num_batches = int(np.ceil(x.shape[0] / batch_size))

        for epoch in range(epochs):
            print(f"\n=== [Epoch {epoch + 1}] ì‹œì‘ ===")
            indices = np.random.permutation(x.shape[0])
            x, y = x[indices], y[indices]
            
            for batch in range(num_batches):
                start = batch * batch_size
                end = min(start + batch_size, x.shape[0])
                batch_x, batch_y = x[start:end], y[start:end]
                batch_loss = 0

                for i in range(batch_x.shape[0]):
                    xi = batch_x[i].reshape(1, -1)
                    yi = batch_y[i].reshape(1, -1)

                    # âœ… ê³ ì† ìˆœì „íŒŒ ìˆ˜í–‰
                    output = self.fast_forward(xi)

                    # ì†ì‹¤ ë° ë©”íŠ¸ë¦­ ê³„ì‚°
                    loss_val, metric_val = self.compute_loss_and_metrics(output, yi)
                    batch_loss += loss_val

                    # âš ï¸ ì—­ì „íŒŒ & ì—…ë°ì´íŠ¸ ë¡œì§ì€ ì¶”í›„ ë³„ë„ êµ¬í˜„ í•„ìš”
                    # self.backward_and_update(...)

                print(f"[Batch {batch + 1}] í‰ê·  ì†ì‹¤: {batch_loss / batch_x.shape[0]}")

    def fast_backward(self, y_pred, y_true):
        grad = self.loss.grad(y_true, y_pred).astype(np.float32)  # dL/dy

        for plan in reversed(self.backward_plan):
            if plan["type"] == "activation":
                z = plan["input"].astype(np.float32)
                grad = self._activation_backward(plan["activation"], z, grad)

            elif plan["type"] == "dense":
                A = plan["input"].astype(np.float32)
                W = plan["weight"].astype(np.float32)

                # dW = A.T @ grad
                dW = matrix_ops.matrix_multiply(A.T, grad)
                # dA = grad @ W.T
                dA = matrix_ops.matrix_multiply(grad, W.T)
                # db = grad.sum(axis=0, keepdims=True)
                db = np.sum(grad, axis=0, keepdims=True)

                plan["grad_W"] = dW
                plan["grad_b"] = db
                grad = dA  # propagate to next layer

import logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

import sys, os
import json
import typing
import logging
import numpy as np
import cupy as cp

sys.path.append("C:/Users/owner/Desktop/AI_framework-dev/dev/backend/graph_executor/test")

from dev.layers.layer import Layer
from dev.backend.backend_ops.losses import losses as cuda_losses
from dev.backend.backend_ops.optimizers import optimizers
from dev.backend.backend_ops.metrics import metrics

# CUDA ì—°ë™ Pybind11 ëª¨ë“ˆ
import graph_executor as ge
OpStruct = ge.OpStruct
Shape = ge.Shape

# ìƒìˆ˜ ì •ì˜
INPUT_ID = "input"

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class Sequential:
    def __init__(self, layers=None, trainable=True, name=None, input_shape=None):
        self.built = False
        self._layers = []
        self.input_shape = input_shape
        self.trainable = trainable
        self.name = name
        self.output_var = None

        # ë””ë°”ì´ìŠ¤ ìƒíƒœ
        self._device_ready = False     # íŒŒë¼ë¯¸í„°/ì˜µí‹°ë§ˆì´ì € ë²„í¼ ì¤€ë¹„ ì—¬ë¶€
        self.opt_buffers = {}          # {name: {"velocity": cp, "m": cp, "v": cp}}

        # ëŸ¬ë‹ ì„¤ì •
        self.loss_type = None
        self.optimizer_type = None
        self.learning_rate = None

        # ê·¸ë˜í”„/ì…°ì´í”„
        self.E_raw = []
        self.E = []
        self.shapes = {}
        self.weights = {}
        self.biases = {}

        # ì†ì‹¤ ì¶œë ¥ ID (compileì—ì„œ ì„¤ì •)
        self.loss_output_id = None

        # ê¸€ë¡œë²Œ ìŠ¤í…
        self.global_step = 1

    def add(self, layer):
        if not isinstance(layer, Layer):
            raise ValueError("Only instances of Layer can be added.")
        if self._layers:
            prev = self._layers[-1]
            input_shape = prev.output_shape
            if input_shape:
                layer.input_shape = input_shape
                layer.build(input_shape)
        elif layer.input_shape is not None:
            layer.build(layer.input_shape)
        else:
            raise RuntimeError("ì²« ë²ˆì§¸ ë ˆì´ì–´ëŠ” input_shapeë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        self._layers.append(layer)
        logger.info(f"âœ… ë ˆì´ì–´ ì¶”ê°€ë¨: {layer.__class__.__name__} (input_shape={layer.input_shape}, output_shape={layer.output_shape})")

    def compile(self, optimizer='sgd', loss='mse', p_metrics='mse', learning_rate=0.001):
        self.E_raw = []
        self.weights = {}
        self.biases = {}
        self.metric_type = p_metrics
        self.shapes = {}

        input_id = INPUT_ID
        current_shape = self.input_shape

        # ë ˆì´ì–´ë¥¼ ìˆœíšŒí•˜ë©° ê·¸ë˜í”„ êµ¬ì„±
        for i, layer in enumerate(self._layers):
            if i == 0:
                if not layer.input_shape:
                    raise ValueError("ì²« ë²ˆì§¸ ë ˆì´ì–´ì— input_shapeê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                layer.build(layer.input_shape)
                current_shape = layer.compute_output_shape(layer.input_shape)
            else:
                layer.input_shape = current_shape
                layer.build(current_shape)
                current_shape = layer.compute_output_shape(current_shape)

            e_block, w, b, output_id, shape_map = layer.to_e_matrix(input_id)
            self.E_raw.extend(e_block)
            self.weights.update(w)
            self.biases.update(b)
            self.shapes.update(shape_map)
            input_id = output_id

        # ëª¨ë¸ ìµœì¢… ì¶œë ¥(ì†ì‹¤ ì´ì „) ID
        self.output_var = input_id

        # í•™ìŠµ ì„¤ì •
        self.loss_type = loss
        self.optimizer_type = optimizer
        self.learning_rate = learning_rate

        # âœ… ë§ˆì§€ë§‰ì— ì†ì‹¤ ë…¸ë“œ ì¶”ê°€ (labelì€ param_idë¡œ)
        extra = ge.OpExtraParams()
        extra.label_id = "y_true"
        extra.loss_type = self.loss_type

        self.E_raw.append({
            "op_type": ge.OpType.LOSS,
            "input_id": self.output_var,   # ì†ì‹¤ì˜ ì…ë ¥ì€ ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥
            "param_id": "y_true",
            "output_id": "loss",
            "extra_params": extra
        })
        # ğŸ”¥ ì—­ì „íŒŒ/ì†ì‹¤ ê³„ì‚°ì˜ ê¸°ì¤€ì´ ë˜ëŠ” ì¶œë ¥ IDë¥¼ ë³´ê´€
        self.loss_output_id = "loss"

        self.built = True

        # âœ… OpStruct ë³€í™˜
        self.E = []
        for op in self.E_raw:
            extra = op.get("extra_params", ge.OpExtraParams())
            param_id = op.get("param_id", "") or ""
            node = ge.OpStruct(
                ge.OpType(op["op_type"]),   # int -> enum ìºìŠ¤íŒ…
                str(op["input_id"]),
                str(param_id),
                str(op["output_id"]),
                extra
            )
            self.E.append(node)

    # ë‚´ë¶€ ìœ í‹¸: ë””ë°”ì´ìŠ¤ íŒŒë¼ë¯¸í„°/ì˜µí‹°ë§ˆì´ì € ë²„í¼ ì´ˆê¸°í™”(1íšŒ)
    def _ensure_device_state(self):
        if self._device_ready:
            return

        # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë²„í¼ ì¤€ë¹„ (í•„ìš” ì‹œ ìƒì„±)
        for name in list(self.weights.keys()) + list(self.biases.keys()):
            param = self.weights.get(name) if name in self.weights else self.biases[name]
            if name not in self.opt_buffers:
                self.opt_buffers[name] = {
                    "velocity": cp.zeros_like(param),
                    "m": cp.zeros_like(param),
                    "v": cp.zeros_like(param)
                }

        self._device_ready = True

    # OptimizerType ë§¤í•‘
    def _opt_type_enum(self):
        s = (self.optimizer_type or "sgd").lower()
        if s == "sgd":
            return ge.OptimizerType.SGD
        if s == "momentum":
            return ge.OptimizerType.MOMENTUM
        if s == "adam":
            return ge.OptimizerType.ADAM
        raise ValueError(f"Unsupported optimizer type: {self.optimizer_type}")

    # Keras ìŠ¤íƒ€ì¼ í•œ ë°°ì¹˜ í•™ìŠµ (í¸ì˜ìš©)
    def train_on_batch(self, x, y):
        if not self.built:
            raise RuntimeError("âœ… ëª¨ë¸ì´ ì»´íŒŒì¼ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € compile()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")
        self._ensure_device_state()

        x_cp = cp.asarray(x, dtype=cp.float32)
        y_cp = cp.asarray(y, dtype=cp.float32)
        batch_size_actual = x_cp.shape[0]

        # ì…ë ¥/ë¼ë²¨ + íŒŒë¼ë¯¸í„° í¬ì¸í„° ë§µ
        tensor_ptrs = {
            "input": x_cp.data.ptr,
            "y_true": y_cp.data.ptr,
        }
        for name, arr in self.weights.items():
            tensor_ptrs[name] = arr.data.ptr
        for name, arr in self.biases.items():
            tensor_ptrs[name] = arr.data.ptr

        # ì˜µí‹°ë§ˆ ìƒíƒœ í¬ì¸í„° ë§µ
        velocity_ptrs = {n: buf["velocity"].data.ptr for n, buf in self.opt_buffers.items()}
        m_ptrs        = {n: buf["m"].data.ptr        for n, buf in self.opt_buffers.items()}
        v_ptrs        = {n: buf["v"].data.ptr        for n, buf in self.opt_buffers.items()}

        loss_val = ge.train_step_entry(
            E=self.E,
            tensors=tensor_ptrs,
            shapes=self.shapes,
            # ğŸ”¥ ì—­ì „íŒŒ ì‹œì‘ì ì€ ì†ì‹¤ ì¶œë ¥
            final_output_id=(self.loss_output_id or self.output_var),
            label_tensor_id="y_true",
            loss_type=self.loss_type,
            batch_size=batch_size_actual,
            opt_type=self._opt_type_enum(),
            lr=self.learning_rate,
            beta1=0.9, beta2=0.999, eps=1e-8,
            timestep=self.global_step,
            velocity_ptrs=velocity_ptrs,
            m_ptrs=m_ptrs,
            v_ptrs=v_ptrs
        )

        self.global_step += 1
        return float(loss_val)

    def fit(self, x=None, y=None, epochs=1, batch_size=-1, verbose=0):
        if not self.built:
            raise RuntimeError("âœ… ëª¨ë¸ì´ ì»´íŒŒì¼ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € compile()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

        if batch_size == -1 or batch_size < x.shape[0]:
            batch_size = x.shape[0]

        self._ensure_device_state()

        for epoch in range(epochs):
            epoch_loss = 0.0
            batch_count = 0

            indices = np.random.permutation(x.shape[0])
            x = x[indices]
            y = y[indices]

            x_cp = cp.asarray(x, dtype=cp.float32)
            y_cp = cp.asarray(y, dtype=cp.float32)

            for i in range(0, x_cp.shape[0], batch_size):
                batch_x = x_cp[i:i + batch_size]
                batch_y = y_cp[i:i + batch_size]

                loss_val = self.train_on_batch(batch_x, batch_y)

                epoch_loss += float(loss_val)
                batch_count += 1

            if verbose and batch_count > 0:
                avg_loss = epoch_loss / batch_count
                logger.info(f"[Epoch {epoch + 1}] í‰ê·  ì†ì‹¤: {avg_loss:.6f}")

    def predict(self, x: np.ndarray) -> np.ndarray:
        if not self.built:
            raise RuntimeError("âœ… ëª¨ë¸ì´ ì»´íŒŒì¼ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € compile()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

        self._ensure_device_state()

        x_cp = cp.asarray(x, dtype=cp.float32)
        batch_size = x_cp.shape[0]

        # ì…ë ¥ + íŒŒë¼ë¯¸í„° í¬ì¸í„°
        tensor_ptrs = {"input": x_cp.data.ptr}
        for name, arr in self.weights.items():
            tensor_ptrs[name] = arr.data.ptr
        for name, arr in self.biases.items():
            tensor_ptrs[name] = arr.data.ptr

        # ìµœì¢…(ì†ì‹¤ ì´ì „) ì¶œë ¥ shape
        out_shape = self.shapes[self.output_var]
        output_host = np.zeros((batch_size, out_shape.cols), dtype=np.float32)

        ge.run_graph_forward_entry(
            E=self.E,
            tensors=tensor_ptrs,
            shapes=self.shapes,
            out_host=output_host,
            final_output_id=self.output_var,   # predictëŠ” ì†ì‹¤ ì´ì „ ì¶œë ¥ì´ í•„ìš”
            batch_size=batch_size
        )
        return output_host

    def evaluate(self, x: np.ndarray, y: np.ndarray) -> float:
        if not self.built:
            raise RuntimeError("âœ… ëª¨ë¸ì´ ì»´íŒŒì¼ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € compile()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

        self._ensure_device_state()

        x_cp = cp.asarray(x, dtype=cp.float32)
        y_cp = cp.asarray(y, dtype=cp.float32)
        batch_size = x_cp.shape[0]

        tensor_ptrs = {"input": x_cp.data.ptr, "y_true": y_cp.data.ptr}
        for name, arr in self.weights.items():
            tensor_ptrs[name] = arr.data.ptr
        for name, arr in self.biases.items():
            tensor_ptrs[name] = arr.data.ptr

        # ğŸ”¥ ì†ì‹¤ ê¸°ì¤€ìœ¼ë¡œ fwd+loss ìˆ˜í–‰
        loss_val = ge.run_graph_with_loss_entry(
            E=self.E,
            tensors=tensor_ptrs,
            shapes=self.shapes,
            final_output_id=(self.loss_output_id or self.output_var),
            label_tensor_id="y_true",
            loss_type=self.loss_type,
            batch_size=batch_size
        )
        return float(loss_val)

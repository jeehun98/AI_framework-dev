import typing
import json
import numpy as np

from dev.layers.layer import Layer
from dev.losses import losses_mapping
from dev import optimizers
from dev.backend.backend_ops.losses import losses as cuda_losses
from dev import metrics
from dev.node.node import Node
from dev.graph_engine.core_graph import Cal_graph


class Sequential(Node):
    def __new__(cls, *args, **kwargs):
        return typing.cast(cls, super().__new__(cls))

    def __init__(self, layers=None, trainable=True, name=None):
        self.built = False
        self._layers = []
        self.cal_graph = Cal_graph()
        self.loss_node_list = []

    def get_config(self):
        sequential_config = {'name': 'sequential'}
        layer_configs = [layer.get_config() for layer in self._layers]
        return {**sequential_config, "layers": layer_configs}

    def add(self, layer):
        if not isinstance(layer, Layer):
            raise ValueError("Only instances of Layer can be added.")

        if self._layers:
            previous_layer = self._layers[-1]
            input_shape = previous_layer.output_shape
            if input_shape is not None:
                layer.build(input_shape)
            elif hasattr(layer, "input_shape") and layer.input_shape is not None:
                layer.build(layer.input_shape)
            else:
                raise RuntimeError(f"ë ˆì´ì–´ {layer.layer_name}ì˜ ì…ë ¥ shapeì„ ì¶”ë¡ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            if hasattr(layer, "input_shape") and layer.input_shape is not None:
                layer.build(layer.input_shape)
            else:
                raise RuntimeError("ì²« ë²ˆì§¸ ë ˆì´ì–´ëŠ” input_shapeë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

        print(f"âœ… ë ˆì´ì–´ ì¶”ê°€ë¨: {layer.__class__.__name__} (input_shape={layer.input_shape}, output_shape={layer.output_shape})")
        self._layers.append(layer)

    def build(self):
        self.input_shape = self._layers[0].input_shape

    def get_build_config(self):
        return {"input_shape": self.input_shape}

    def compile(self, optimizer=None, loss=None, p_metrics=None, learning_rate=0.001):
        self.optimizer = optimizers.get(optimizer, learning_rate=learning_rate)
        self.loss = cuda_losses.get(loss)
        self.loss_name = loss  # ì†ì‹¤ í•¨ìˆ˜ ì´ë¦„ ì €ì¥
        self.metric = metrics.get(p_metrics)
        self.build()
        self.get_weight()


    def get_compile_config(self):
        return {
            "optimizer": self.optimizer.get_config(),
            "loss": self.loss.get_config(),
            "metrics": self.metric.get_config(),
        }

    def get_weight(self):
        self.weights = [layer.weights for layer in self._layers if hasattr(layer, 'weights')]

    def serialize_model(self):
        compile_config = self.get_compile_config()
        model_config = self.get_config()
        build_config = self.get_build_config()

        weights = []
        for layer in self._layers:
            layer_weights = layer.get_weights()
            serialized_weights = [w.tolist() for w in layer_weights]
            weights.append(serialized_weights)

        model_data = {
            "compile_config": compile_config,
            "model_config": model_config,
            "build_config": build_config,
            "weights": weights,
        }

        return json.dumps(model_data)

    def predict(self, data):
        output = data
        for i, layer in enumerate(self._layers):
            output = layer.call(output)
        return output

    def forward_pass(self, input_data):
        output = input_data
        print(f"[SHAPE TRACE] Input: {output.shape}")
        for i, layer in enumerate(self._layers):
            output = layer.call(output)
            print(f"[SHAPE TRACE] Layer {i}: {layer.__class__.__name__} â†’ output: {output.shape}")
        return output

    def connect_loss_graph(self):
        if self.loss_node_list:
            print(f"[DEBUG] Loss Graph ì—°ê²°: ì¶œë ¥ ìœ ë‹› {len(self.loss_leaf_nodes)}ê°œ")

            # ì¶œë ¥ì¸µê³¼ ì†ì‹¤ í•¨ìˆ˜ ê·¸ë˜í”„ ì—°ê²°
            self.cal_graph.root_node_list = self.cal_graph.connect_graphs(
                self.cal_graph.root_node_list, self.loss_leaf_nodes
            )

            # ìµœì¢… ë£¨íŠ¸ëŠ” ì†ì‹¤ í•¨ìˆ˜ ë£¨íŠ¸ë¡œ ë®ì–´ì“°ê¸°
            self.cal_graph.root_node_list = self.loss_node_list[:]

    def compute_loss_and_metrics(self, y_pred_array, y_true_array):
        # 1ï¸âƒ£ CUDA ì—°ì‚°
        self.loss_value = self.loss(y_true_array, y_pred_array)

        print(y_pred_array, "ë­ì•¼ ì´ê²Œ")

        # 2ï¸âƒ£ ê³„ì‚° ê·¸ë˜í”„ ìƒì„± (ì¶œë ¥ ìœ ë‹› ìˆ˜ ê¸°ë°˜)
        num_outputs = y_pred_array.shape[1]  # (1, N) í˜•íƒœ ê¸°ì¤€
        try:
            builder = getattr(losses_mapping, f"{self.loss_name}_graph")
        except AttributeError:
            raise NotImplementedError(f"{self.loss_name} ê³„ì‚° ê·¸ë˜í”„ ë¯¸ì§€ì›")

        if self.loss_name == "categorical_crossentropy":
            loss_root, leaf_nodes = builder(num_classes=num_outputs)
        else:
            loss_root, leaf_nodes = builder(num_outputs=num_outputs)

        self.loss_node_list = [loss_root]
        self.loss_leaf_nodes = leaf_nodes

        # 3ï¸âƒ£ ë©”íŠ¸ë¦­ ê³„ì‚°
        self.metric_value = self.metric(y_pred_array, y_true_array)

        return self.loss_value




    def fit(self, x=None, y=None, epochs=1, batch_size=-1):
        if batch_size == -1 or batch_size < x.shape[0]:
            batch_size = x.shape[0]
        batch_counts = int(np.ceil(x.shape[0] / batch_size))

        for epoch in range(epochs):
            print(f"\n=== [Epoch {epoch + 1}] ì‹œì‘ ===")

            indices = np.random.permutation(x.shape[0])
            x = x[indices]
            y = y[indices]

            for batch_idx in range(batch_counts):
                print(f"\n[Batch {batch_idx + 1}] ì²˜ë¦¬ ì‹œì‘")

                start = batch_idx * batch_size
                end = min(start + batch_size, x.shape[0])
                batch_x = x[start:end]
                batch_y = y[start:end]
                batch_datas = batch_x.shape[0]
                batch_loss_sum = 0

                for data_idx in range(batch_datas):
                    input_data = batch_x[data_idx]
                    target = batch_y[data_idx]

                    print(f"\n[SHAPE TRACE] === Sample {data_idx + 1} ===")
                    print(f"[SHAPE TRACE] Input: {input_data.shape}")

                    output = input_data
                    prev_root_nodes = None

                    for i, layer in enumerate(self._layers):
                        print(f"[DEBUG] Layer {i}: {layer.__class__.__name__} call() ì‹¤í–‰")
                        output = layer.call(output)
                        print(f"[SHAPE TRACE] Layer {i}: {layer.__class__.__name__} â†’ output: {output.shape}")

                        if hasattr(layer, "root_node_list") and layer.root_node_list:
                            if prev_root_nodes is None:
                                # ì²« ë ˆì´ì–´: ë‹¨ìˆœíˆ root ì„¤ì •
                                self.cal_graph.root_node_list = layer.root_node_list[:]
                            else:
                                # âœ… ì—°ê²°ë§Œ ìˆ˜í–‰
                                self.cal_graph.connect_graphs(prev_root_nodes, layer.leaf_node_list)
    
                                # âœ… ë£¨íŠ¸ëŠ” í•­ìƒ "í˜„ì¬ ë ˆì´ì–´ì˜ root"ë¡œ ê°±ì‹ 
                                self.cal_graph.root_node_list = layer.root_node_list[:]

                                """
                                for i in range(len(self.cal_graph.root_node_list)):
                                    self.cal_graph.root_node_list[i].print_tree()
                                    print("ê° ê³„ì‚° ê·¸ë˜í”„ í™•ì¸ìš©")
                                """
                                
                                

                            prev_root_nodes = layer.root_node_list[:]

                    
                    output = np.array(output).reshape(1, -1)
                    target = np.array(target).reshape(1, -1)

                    print("[DEBUG] ì†ì‹¤ ë° ë©”íŠ¸ë¦­ ê³„ì‚° ì‹œì‘")
                    loss_value = self.compute_loss_and_metrics(output, target)
                    print("[DEBUG] ì†ì‹¤ ë° ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ")

                    self.connect_loss_graph()

                    self.cal_graph.print_graph()

                    batch_loss_sum += loss_value

                batch_loss = batch_loss_sum / batch_datas
                print(f"[Batch {batch_idx + 1}] í‰ê·  ì†ì‹¤: {batch_loss}")

                print("[DEBUG] ì—­ì „íŒŒ ì‹œì‘")
                for root_node in self.cal_graph.root_node_list:
                    self.backpropagate(root_node)
                print("[DEBUG] ì—­ì „íŒŒ ì™„ë£Œ")

                print("[DEBUG] ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì‹œì‘")
                for root_node in self.cal_graph.root_node_list:
                    self.weight_update(root_node, batch_datas, self.optimizer)
                print("[DEBUG] ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

                print("[DEBUG] ê³„ì‚° ê·¸ë˜í”„ ì¶œë ¥:")
                self.cal_graph.print_graph()

            total_loss = 0
            for i in range(x.shape[0]):
                pred = self.predict(x[i])
                pred = np.array(pred).reshape(1, -1)
                loss = self.compute_loss_and_metrics(pred, y[i].reshape(1, -1))
                total_loss += loss

            print(f"\nğŸ“Š [Epoch {epoch + 1}] ì „ì²´ í‰ê·  ì†ì‹¤: {total_loss / x.shape[0]}")

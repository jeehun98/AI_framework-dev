import typing
import json
import numpy as np
import cupy as cp
import sys
import os
import logging

from dev.layers.layer import Layer
from dev.backend.backend_ops.losses import losses as cuda_losses
from dev.backend.backend_ops.optimizers import optimizers
from dev.backend.backend_ops.metrics import metrics

sys.path.append("C:/Users/owner/Desktop/AI_framework-dev/dev/backend/graph_executor/test")

# CUDA ì—°ë™ Pybind11 ëª¨ë“ˆ
import graph_executor as ge
OpStruct = ge.OpStruct
Shape = ge.Shape

# ìƒìˆ˜ ì •ì˜
INPUT_ID = "input"
GRAPH_FILE = "compiled_graph.npz"

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


class Sequential:
    def __init__(self, layers=None, trainable=True, name=None, input_shape=None):
        self.built = False
        self._layers = []
        self.input_shape = input_shape
        self.trainable = trainable
        self.name = name
        self.output_var = None

    def add(self, layer):
        if not isinstance(layer, Layer):
            raise ValueError("Only instances of Layer can be added.")
        if self._layers:
            prev = self._layers[-1]
            input_shape = prev.output_shape
            if input_shape:
                layer.input_shape = input_shape
                layer.build(input_shape)
        elif layer.input_shape is not None:
            layer.build(layer.input_shape)
        else:
            raise RuntimeError("ì²« ë²ˆì§¸ ë ˆì´ì–´ëŠ” input_shapeë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        self._layers.append(layer)
        logger.info(f"âœ… ë ˆì´ì–´ ì¶”ê°€ë¨: {layer.__class__.__name__} (input_shape={layer.input_shape}, output_shape={layer.output_shape})")

    def compile(self, optimizer='sgd', loss='mse', p_metrics='mse', learning_rate=0.001):
        self.E_raw = []
        self.weights = {}
        self.biases = {}
        self.metric_type = p_metrics
        self.shapes = {}

        input_id = INPUT_ID
        current_shape = self.input_shape

        for i, layer in enumerate(self._layers):
            if i == 0:
                if not layer.input_shape:
                    raise ValueError("ì²« ë²ˆì§¸ ë ˆì´ì–´ì— input_shapeê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                layer.build(layer.input_shape)
                current_shape = layer.compute_output_shape(layer.input_shape)
            else:
                layer.input_shape = current_shape
                layer.build(current_shape)
                current_shape = layer.compute_output_shape(current_shape)

            e_block, w, b, output_id, shape_map = layer.to_e_matrix(input_id)
            self.E_raw.extend(e_block)
            self.weights.update(w)
            self.biases.update(b)
            self.shapes.update(shape_map)
            input_id = output_id

        self.output_var = input_id

        # âœ… CUDA ëŸ°íƒ€ì„ìš© í•™ìŠµ ì„¤ì • ì €ì¥
        self.loss_type = loss
        self.optimizer_type = optimizer
        self.learning_rate = learning_rate

        # âœ… ë§ˆì§€ë§‰ì— ì†ì‹¤ ë…¸ë“œ ì¶”ê°€ (labelì€ param_idì— ë‹´ì•„ì¤Œ)
        extra = ge.OpExtraParams()
        extra.label_id = "y_true"  # âœ… ğŸ”¥ í•µì‹¬ ì¶”ê°€
        extra.loss_type = self.loss_type

        self.E_raw.append({
            "op_type": ge.OpType.LOSS,
            "input_id": self.output_var,
            "param_id": "y_true",
            "output_id": "loss",
            "extra_params": extra     # ğŸ”‘ ì´ê±¸ ê°™ì´ ë„£ì–´ì•¼ Pybind11ì„ í†µí•´ ì „ë‹¬ë¨
        })


        self.built = True

        # âœ… OpStruct ë³€í™˜
        self.E = []
        for op in self.E_raw:
            extra = op.get("extra_params", ge.OpExtraParams())
            param_id = op.get("param_id", "") or ""
            node = ge.OpStruct(
                int(op["op_type"]),
                str(op["input_id"]),
                str(param_id),
                str(op["output_id"]),
                extra
            )
            self.E.append(node)

    def fit(self, x=None, y=None, epochs=1, batch_size=-1):
        if batch_size == -1 or batch_size < x.shape[0]:
            batch_size = x.shape[0]

        self.global_step = 1  # Adam ë“±ì—ì„œ í•„ìš”í•œ timestep

        # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë²„í¼ ì´ˆê¸°í™”
        if not hasattr(self, "opt_buffers"):
            self.opt_buffers = {}

        for epoch in range(epochs):
            logger.info(f"\n=== [Epoch {epoch + 1}] ì‹œì‘ ===")

            indices = np.random.permutation(x.shape[0])
            x = x[indices]
            y = y[indices]

            x_cp = cp.asarray(x, dtype=cp.float32)
            y_cp = cp.asarray(y, dtype=cp.float32)

            for i in range(0, x_cp.shape[0], batch_size):
                batch_x = x_cp[i:i + batch_size]
                batch_y = y_cp[i:i + batch_size]
                batch_size_actual = batch_x.shape[0]

                tensor_ptrs = {"input": batch_x.data.ptr, "y_true": batch_y.data.ptr}
                self.tensor_map = {"input": batch_x.copy(), "y_true": batch_y.copy()}

                # ê°€ì¤‘ì¹˜ ë° í¸í–¥ í¬ì¸í„° ë“±ë¡
                for name, arr in self.weights.items():
                    cp_arr = cp.asarray(arr, dtype=cp.float32)
                    tensor_ptrs[name] = cp_arr.data.ptr
                    self.tensor_map[name] = cp_arr.copy()

                for name, arr in self.biases.items():
                    cp_arr = cp.asarray(arr, dtype=cp.float32)
                    tensor_ptrs[name] = cp_arr.data.ptr
                    self.tensor_map[name] = cp_arr.copy()

                # ì¤‘ê°„ ë³€ìˆ˜ìš© ë©”ëª¨ë¦¬ í™•ë³´
                for var, shape in self.shapes.items():
                    if var not in tensor_ptrs:
                        buf = cp.empty((shape.rows, shape.cols), dtype=cp.float32)
                        tensor_ptrs[var] = buf.data.ptr
                        self.tensor_map[var] = buf

                # âœ… Forward + Loss
                loss_val = ge.run_graph_with_loss_entry(
                    E=self.E,
                    tensors=tensor_ptrs,
                    shapes=self.shapes,
                    final_output_id="loss",        # ğŸ”µ ì†ì‹¤ ë…¸ë“œ ID ëª…ì‹œ
                    label_tensor_id="y_true",
                    loss_type=self.loss_type,
                    batch_size=batch_size_actual
                )

                # âœ… 1. ì†ì‹¤ ë…¸ë“œì— ëŒ€í•œ ì´ˆê¸° gradient (dL/dL = 1)
                loss_grad = cp.array([1.0], dtype=cp.float32)

                # âœ… 2. ì—­ì „íŒŒ ì‹œì‘ì€ "loss" ë…¸ë“œë¡œë¶€í„°
                grad_ptrs = {
                    "loss": loss_grad.data.ptr
                }

                # âœ… 3. Backward ì‹¤í–‰ (output_id = "loss")
                grad_map = ge.run_graph_backward_entry(
                    E=self.E,
                    tensors=tensor_ptrs,
                    shapes=self.shapes,
                    gradients=grad_ptrs,
                    final_output_id="loss",
                    batch_size=batch_size_actual
                )

                # âœ… Optimizer ì ìš©
                for name in list(self.weights.keys()) + list(self.biases.keys()):
                    param = self.tensor_map[name]
                    grad_ptr = grad_map.get(name, 0)
                    if grad_ptr == 0:
                        continue

                    grad = cp.ndarray(param.shape, dtype=cp.float32,
                                    memptr=cp.cuda.MemoryPointer(
                                        cp.cuda.UnownedMemory(grad_ptr, param.nbytes, None), 0))

                    # ì˜µí‹°ë§ˆì´ì € ë²„í¼ í™•ë³´
                    if name not in self.opt_buffers:
                        self.opt_buffers[name] = {
                            "velocity": cp.zeros_like(param),
                            "m": cp.zeros_like(param),
                            "v": cp.zeros_like(param)
                        }

                    velocity = self.opt_buffers[name]["velocity"]
                    m = self.opt_buffers[name]["m"]
                    v = self.opt_buffers[name]["v"]

                    # ì˜µí‹°ë§ˆì´ì € íƒ€ì… enum
                    opt_type_str = self.optimizer_type.lower()
                    if opt_type_str == "sgd":
                        opt_type_enum = ge.OptimizerType.SGD
                    elif opt_type_str == "momentum":
                        opt_type_enum = ge.OptimizerType.MOMENTUM
                    elif opt_type_str == "adam":
                        opt_type_enum = ge.OptimizerType.ADAM
                    else:
                        raise ValueError(f"Unsupported optimizer type: {self.optimizer_type}")

                    # CUDA Optimizer ì‹¤í–‰
                    ge.optimizer_update(
                        param_ptr=param.data.ptr,
                        grad_ptr=grad.data.ptr,
                        velocity_ptr=velocity.data.ptr,
                        m_ptr=m.data.ptr,
                        v_ptr=v.data.ptr,
                        lr=self.learning_rate,
                        beta1=0.9,
                        beta2=0.999,
                        eps=1e-8,
                        size=param.size,
                        opt_type=opt_type_enum,
                        timestep=self.global_step
                    )

                    # NaN ì²´í¬ ë¡œê·¸ (ì˜µì…˜)
                    if cp.isnan(param).any():
                        logger.warning(f"[NaN] ë°œìƒ: {name} ì—…ë°ì´íŠ¸ í›„ NaN í¬í•¨")

                    # ì—…ë°ì´íŠ¸ëœ íŒŒë¼ë¯¸í„° ì €ì¥
                    if name in self.weights:
                        self.weights[name] = param
                    elif name in self.biases:
                        self.biases[name] = param

                    # ğŸ” ì—…ë°ì´íŠ¸ í›„ weight í‰ê·  ë¡œê·¸
                    if "w" in name:
                        logger.debug(f"[Epoch {epoch+1}] {name} mean: {cp.mean(param):.6f}")


                self.global_step += 1
                logger.info(f"[Batch ì™„ë£Œ] ì†ì‹¤: {loss_val:.10f}")

            # Epoch ë§ˆë¬´ë¦¬ weight ë¡œê·¸ (ì„ íƒì )
            for name, param in self.weights.items():
                logger.debug(f"[Epoch {epoch+1}] {name} ìƒ˜í”Œ: {param.ravel()[:5]}")


    def predict(self, x: np.ndarray) -> np.ndarray:
        if not self.built:
            raise RuntimeError("âœ… ëª¨ë¸ì´ ì»´íŒŒì¼ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € compile()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

        x_cp = cp.asarray(x, dtype=cp.float32)
        batch_size = x_cp.shape[0]

        # 1. ì…ë ¥ í…ì„œ í¬ì¸í„° ì¤€ë¹„
        tensor_ptrs = {"input": x_cp.data.ptr}
        self.tensor_map = {"input": x_cp}

        # 2. ê°€ì¤‘ì¹˜ & í¸í–¥ ì¤€ë¹„
        for name, arr in self.weights.items():
            cp_arr = cp.asarray(arr, dtype=cp.float32)
            tensor_ptrs[name] = cp_arr.data.ptr
            self.tensor_map[name] = cp_arr

        for name, arr in self.biases.items():
            cp_arr = cp.asarray(arr, dtype=cp.float32)
            tensor_ptrs[name] = cp_arr.data.ptr
            self.tensor_map[name] = cp_arr

        # 3. ë‚˜ë¨¸ì§€ ì¤‘ê°„ í…ì„œë“¤ ì´ˆê¸°í™”
        for var, shape in self.shapes.items():
            if var not in tensor_ptrs:
                buf = cp.empty((shape.rows, shape.cols), dtype=cp.float32)
                tensor_ptrs[var] = buf.data.ptr
                self.tensor_map[var] = buf

        # 4. ì¶œë ¥ shape í™•ì¸ ë° ì´ˆê¸°í™”
        out_shape = self.shapes[self.output_var]
        
        output_host = np.zeros((batch_size, out_shape.cols), dtype=np.float32)


        # 5. CUDA forward ì‹¤í–‰
        ge.run_graph_forward_entry(
            E=self.E,
            tensors=tensor_ptrs,
            shapes=self.shapes,
            out_host=output_host,
            final_output_id=self.output_var,
            batch_size=batch_size
        )

        # 6. ê²°ê³¼ ë°˜í™˜ (CPU ndarray)
        return output_host

    def evaluate(self, x: np.ndarray, y: np.ndarray) -> float:
        if not self.built:
            raise RuntimeError("âœ… ëª¨ë¸ì´ ì»´íŒŒì¼ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € compile()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

        x_cp = cp.asarray(x, dtype=cp.float32)
        y_cp = cp.asarray(y, dtype=cp.float32)
        batch_size = x_cp.shape[0]

        # 1. ì…ë ¥ ë° ì •ë‹µ í…ì„œ í¬ì¸í„°
        tensor_ptrs = {
            "input": x_cp.data.ptr,
            "y_true": y_cp.data.ptr
        }
        self.tensor_map = {
            "input": x_cp,
            "y_true": y_cp
        }

        # 2. ê°€ì¤‘ì¹˜ ë° í¸í–¥ í¬ì¸í„°
        for name, arr in self.weights.items():
            cp_arr = cp.asarray(arr, dtype=cp.float32)
            tensor_ptrs[name] = cp_arr.data.ptr
            self.tensor_map[name] = cp_arr

        for name, arr in self.biases.items():
            cp_arr = cp.asarray(arr, dtype=cp.float32)
            tensor_ptrs[name] = cp_arr.data.ptr
            self.tensor_map[name] = cp_arr

        # 3. ì¤‘ê°„ í…ì„œ ë²„í¼ ì¤€ë¹„
        for var, shape in self.shapes.items():
            if var not in tensor_ptrs:
                buf = cp.empty((shape.rows, shape.cols), dtype=cp.float32)
                tensor_ptrs[var] = buf.data.ptr
                self.tensor_map[var] = buf

        # ì†ì‹¤ ê³„ì‚° ì „
        if "loss" in self.tensor_map:
            loss_check = self.tensor_map["loss"]
            cp.cuda.runtime.deviceSynchronize()
            logger.debug(f"[Before Loss] loss buffer: {cp.asnumpy(loss_check.ravel()[:4])}")

        # 4. ì†ì‹¤ ê³„ì‚°
        loss_val = ge.run_graph_with_loss_entry(
            E=self.E,
            tensors=tensor_ptrs,
            shapes=self.shapes,
            final_output_id=self.output_var,
            label_tensor_id="y_true",
            loss_type=self.loss_type,
            batch_size=batch_size
        )

        # ì†ì‹¤ ê³„ì‚° í›„
        if "loss" in self.tensor_map:
            loss_check = self.tensor_map["loss"]
            cp.cuda.runtime.deviceSynchronize()
            logger.debug(f"[After Loss] loss buffer: {cp.asnumpy(loss_check.ravel()[:4])}")


        # 5. ì¶œë ¥ ë° ë©”íŠ¸ë¦­ ê³„ì‚°
        output_arr = self.tensor_map[self.output_var]
        y_true_arr = self.tensor_map["y_true"]

        if self.metric_type.lower() == "mse":
            metric_result = metrics.mse(output_arr, y_true_arr)
        elif self.metric_type.lower() == "mae":
            metric_result = metrics.mae(output_arr, y_true_arr)
        elif self.metric_type.lower() == "accuracy":
            metric_result = metrics.accuracy(output_arr, y_true_arr)
        else:
            raise ValueError(f"Unsupported metric type: {self.metric_type}")

        logger.info(f"ğŸ“Š í‰ê°€ ì†ì‹¤: {loss_val:.10f}, ë©”íŠ¸ë¦­({self.metric_type}): {metric_result:.6f}")
        return float(metric_result)

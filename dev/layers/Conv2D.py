import cupy as cp
import numpy as np
from dev.layers.layer import Layer

import sys
sys.path.append("C:/Users/owner/Desktop/AI_framework-dev/dev/backend/graph_executor")
import graph_executor as ge  # Pybind11 ëª¨ë“ˆ

Shape = ge.Shape
OpExtraParams = ge.OpExtraParams


class Conv2D(Layer):
    def __init__(self, filters, kernel_size, strides=(1, 1), padding='valid',
                 activation=None, input_shape=None, name=None, initializer='he',
                 use_bias=True, force_bias_tile=False,  # âœ… ì˜µì…˜ ì¶”ê°€
                 **kwargs):
        super().__init__(name=name, **kwargs)
        self.layer_name = "conv2d"
        self.filters = int(filters)
        self.kernel_size = tuple(map(int, kernel_size))
        self.strides = tuple(map(int, strides))
        self.padding = padding.lower()
        self.activation = activation
        self.initializer = initializer
        self.input_shape = input_shape
        self.use_bias = bool(use_bias)
        self.force_bias_tile = bool(force_bias_tile)  # âœ… ì—”ì§„ì´ ì±„ë„í¸í–¥ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ëª»í•  ë•Œ

        self.name = name or f"conv2d_{id(self)}"
        self.weight_var = f"{self.name}_W"
        self.bias_var = f"{self.name}_b"
        self.output_var = f"{self.name}_out"

        self.weights = None
        self.bias = None

        self.activation_name = (getattr(self.activation, "__name__", str(self.activation)).lower()
                                if self.activation else None)

        # ë‚´ë¶€ ìºì‹œ
        self.out_h = None
        self.out_w = None
        self.pad_h_total = 0
        self.pad_w_total = 0

    def __call__(self, x):
        if not self.built:
            self.build(x.shape)
        return self.call(x)

    def build(self, input_shape):
        # ì…ë ¥ í˜•ìƒ ì •ê·œí™”
        if len(input_shape) == 3:
            input_shape = (*input_shape, 1)  # (b, h, w) â†’ (b, h, w, c)
        if len(input_shape) != 4:
            raise ValueError(f"[Conv2D] build: expected input shape (b, h, w, c), got {input_shape}")

        self.input_shape = tuple(map(int, input_shape))
        b, in_h, in_w, in_c = self.input_shape
        kh, kw = self.kernel_size
        sh, sw = self.strides

        if self.padding not in ("valid", "same"):
            raise ValueError(f"[Conv2D] Unsupported padding: {self.padding}")

        # ì¶œë ¥ í¬ê¸° & SAME íŒ¨ë”© ê³„ì‚°
        if self.padding == 'valid':
            out_h = (in_h - kh) // sh + 1
            out_w = (in_w - kw) // sw + 1
            pad_h_total = 0
            pad_w_total = 0
        else:  # 'same'
            out_h = int(np.ceil(in_h / sh))
            out_w = int(np.ceil(in_w / sw))
            # ì´ íŒ¨ë”© í”½ì…€ ìˆ˜(ìƒ+í•˜ / ì¢Œ+ìš°) â€” í‘œì¤€ conv same ê³µì‹
            pad_h_total = max((out_h - 1) * sh + kh - in_h, 0)
            pad_w_total = max((out_w - 1) * sw + kw - in_w, 0)

        if out_h <= 0 or out_w <= 0:
            raise ValueError(f"[Conv2D] Invalid output shape: {out_h}x{out_w}")

        self.out_h, self.out_w = int(out_h), int(out_w)
        self.pad_h_total, self.pad_w_total = int(pad_h_total), int(pad_w_total)
        self.output_shape = (b, self.out_h, self.out_w, self.filters)

        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (He normal ê¸°ë³¸)
        fan_in = in_c * kh * kw
        if self.initializer == 'ones':
            W = cp.ones((self.filters, in_c, kh, kw), dtype=cp.float32)
        elif self.initializer == 'zeros':
            W = cp.zeros((self.filters, in_c, kh, kw), dtype=cp.float32)
        elif self.initializer == 'xavier':
            limit = cp.sqrt(6.0 / (fan_in + self.filters))
            W = cp.random.uniform(-limit, limit, (self.filters, in_c, kh, kw)).astype(cp.float32)
        elif self.initializer == 'uniform':
            limit = 0.05
            W = cp.random.uniform(-limit, limit, (self.filters, in_c, kh, kw)).astype(cp.float32)
        elif self.initializer == 'normal':
            W = cp.random.normal(0.0, 0.05, (self.filters, in_c, kh, kw)).astype(cp.float32)
        elif self.initializer == 'lecun':
            std = cp.sqrt(1.0 / fan_in)
            W = cp.random.normal(0.0, std, (self.filters, in_c, kh, kw)).astype(cp.float32)
        else:  # 'he' ë° ê¸°ë³¸ê°’
            std = cp.sqrt(2.0 / fan_in)
            W = cp.random.normal(0.0, std, (self.filters, in_c, kh, kw)).astype(cp.float32)

        self.weights = cp.ascontiguousarray(W, dtype=cp.float32)
        if self.use_bias:
            self.bias = cp.zeros((self.filters,), dtype=cp.float32)
        else:
            self.bias = None

        # ì•ˆì „ë§
        if cp.isnan(self.weights).any() or cp.isinf(self.weights).any():
            raise RuntimeError("[Conv2D] Weight contains NaN/Inf")

        self.built = True

    def call(self, x):
        # ì‹¤ì œ forwardëŠ” CUDA backendì—ì„œ ì²˜ë¦¬
        raise NotImplementedError("Forward pass is handled by CUDA backend")

    def backward(self, grad_output):
        raise NotImplementedError("Backward pass is handled by CUDA backend")

    def update(self, optimizer):
        # ê·¸ë˜í”„ ì‹¤í–‰ ê²½ë¡œì—ì„  í˜¸ì¶œë˜ì§€ ì•Šë„ë¡
        raise RuntimeError("Conv2D.update() should not be called; graph_executor handles updates.")

    def compute_output_shape(self, input_shape):
        if len(input_shape) == 3:
            input_shape = (*input_shape, 1)
        b, in_h, in_w, in_c = map(int, input_shape)
        kh, kw = self.kernel_size
        sh, sw = self.strides

        if self.padding == 'valid':
            out_h = (in_h - kh) // sh + 1
            out_w = (in_w - kw) // sw + 1
        elif self.padding == 'same':
            out_h = int(np.ceil(in_h / sh))
            out_w = int(np.ceil(in_w / sw))
        else:
            raise ValueError(f"[Conv2D] Unknown padding: {self.padding}")

        return (b, int(out_h), int(out_w), self.filters)

    def _activation_op_type(self):
        if not self.activation_name:
            return None
        # âœ… í•„ìš”í•œ ê²½ìš° í–¥í›„ í™œì„±í™” í™•ì¥ ê°€ëŠ¥(Leaky/GELU/SILU ë“±)
        act_map = {"relu": 2, "sigmoid": 3, "tanh": 4}
        if self.activation_name not in act_map:
            raise ValueError(f"[Conv2D] Unsupported activation: {self.activation_name}")
        return act_map[self.activation_name]

    def to_e_matrix(self, input_id):
        if self.input_shape is None:
            raise ValueError("[Conv2D] input_shape is None. Did you forget to call build()?")

        weight_id  = self.weight_var
        bias_id    = self.bias_var
        conv_out_id = f"{self.name}_conv"
        output_id   = self.output_var
        preact_id   = f"{self.name}_preact"

        b, in_h, in_w, in_c = self.input_shape
        kh, kw = self.kernel_size
        sh, sw = self.strides
        out_h, out_w = self.out_h, self.out_w

        # âœ… Extra íŒŒë¼ë¯¸í„° ì±„ìš°ê¸° (stride/padding í¬í•¨)
        extra = OpExtraParams()
        extra.batch_size = int(b)
        extra.input_h = int(in_h)
        extra.input_w = int(in_w)
        extra.input_c = int(in_c)
        extra.output_c = int(self.filters)
        extra.kernel_h = int(kh)
        extra.kernel_w = int(kw)
        extra.stride_h = int(sh)          # âœ…
        extra.stride_w = int(sw)          # âœ…
        # SAME paddingì„ ìƒí•˜ì¢Œìš° í•©ê³„ê°€ ì•„ë‹Œ "í•œìª½" ê°’ìœ¼ë¡œ ì—”ì§„ì´ í•´ì„í•œë‹¤ë©´
        # ì•„ë˜ì²˜ëŸ¼ ëŒ€ì¹­ íŒ¨ë”© ì ˆë°˜ì„ ë„£ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.
        extra.padding_h = int(self.pad_h_total // 2)  # âœ…
        extra.padding_w = int(self.pad_w_total // 2)  # âœ…
        extra.use_bias = bool(self.use_bias)

        # e-ë¸”ë¡ êµ¬ì„±
        e_block = [
            {
                "op_type": 6,  # CONV2D
                "input_id":  input_id,
                "param_id":  weight_id,
                "output_id": conv_out_id,
                "extra_params": extra
            }
        ]

        # âœ… Bias ADD: ì—”ì§„ì´ ì±„ë„ë³„ bias ë¸Œë¡œë“œìºìŠ¤íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´ íƒ€ì¼ë§
        biases = {}
        bias_shape = None
        add_out_id = conv_out_id
        if self.use_bias:
            if self.force_bias_tile:
                # (1, F*H*W)ë¡œ íƒ€ì¼ë§
                tile_per_channel = out_h * out_w
                bvec = cp.repeat(self.bias, tile_per_channel).reshape(1, -1).astype(cp.float32)
                biases[bias_id] = cp.ascontiguousarray(bvec)
                bias_shape = Shape(1, int(self.filters * out_h * out_w))
            else:
                # ì±„ë„í¸í–¥ (1, F) â€” ì—”ì§„ì´ (B, F*H*W)ì™€ì˜ ë¸Œë¡œë“œìºìŠ¤íŠ¸ë¥¼ ì§€ì›í•´ì•¼ í•¨
                bvec = self.bias.reshape(1, -1).astype(cp.float32)
                biases[bias_id] = cp.ascontiguousarray(bvec)
                bias_shape = Shape(1, int(self.filters))

            add_out_id = (preact_id if self.activation_name else output_id)
            e_block.append({
                "op_type": 1,  # ADD
                "input_id":  conv_out_id,
                "param_id":  bias_id,
                "output_id": add_out_id,
                "extra_params": extra  # âœ… ì¼ë¶€ ì—”ì§„ì€ ADDì—ë„ extraë¥¼ ì°¸ì¡°(ì˜µì…˜)
            })

        # âœ… í™œì„±í™”
        act_type = self._activation_op_type()
        if act_type is not None:
            e_block.append({
                "op_type": act_type,
                "input_id": add_out_id,
                "param_id": "",
                "output_id": output_id,
                "extra_params": extra
            })

        # Shape ë§¤í•‘  ğŸ”´ ê¸°ì¡´: conv_out_id/output_id ë¥¼ (B, F*H*W)ë¡œ ë‘  â†’ ì˜ëª»
        # âœ… ìˆ˜ì •: conv ì¶œë ¥ì€ ìƒ˜í”Œë‹¹ (rows=F, cols=Hout*Wout)
        shape_map = {
            # ì…ë ¥ì€ (B, Cin*Hin*Win) ê·¸ëŒ€ë¡œ ë‘ì–´ë„ ë¨: ì—”ì§„ì´ extraë¡œ ì›í˜•ë³µì›
            input_id:    Shape(int(b), int(in_c * in_h * in_w)),
            weight_id:   Shape(int(self.filters), int(in_c * kh * kw)),

            # âœ… conv ì¶œë ¥(ìƒ˜í”Œë‹¹ í–‰ë ¬)
            conv_out_id: Shape(int(self.filters), int(out_h * out_w)),
            output_id:   Shape(int(self.filters), int(out_h * out_w)),
        }

        if self.use_bias and bias_shape is not None:
            # bias_shapeëŠ” (1, F) ë˜ëŠ” (1, F*H*W) (force_bias_tileì¼ ë•Œ)
            shape_map[bias_id] = bias_shape

        if self.activation_name:
            shape_map[preact_id] = Shape(int(self.filters), int(out_h * out_w))
        # íŒŒë¼ë¯¸í„° íŒ¨í‚¹(ì—°ì†/float32 ë³´ì¥)
        weights = {
            weight_id: cp.ascontiguousarray(
                self.weights.reshape(self.filters, -1).astype(cp.float32)
            )
        }
        # biasesëŠ” ìœ„ì—ì„œ êµ¬ì„±

        return e_block, weights, biases, output_id, shape_map

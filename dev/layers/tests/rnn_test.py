# rnn_test.py
import os
import sys
import numpy as np

# CUDA DLL ê²½ë¡œ (Windows)
try:
    os.add_dll_directory(r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6\bin")
except Exception:
    pass

# í”„ë¡œì íŠ¸ ê²½ë¡œ
ROOT = "C:/Users/owner/Desktop/AI_framework-dev"
sys.path.insert(0, os.path.abspath(ROOT))
sys.path.append(os.path.join(ROOT, "dev", "backend", "graph_executor", "test"))

# í”„ë ˆì„ì›Œí¬ ì„í¬íŠ¸
from dev.models.sequential import Sequential
from dev.layers.Rnn import RNN           # âœ… ë‹¨ì¼ RNN ì˜¤í¼ë¥¼ ì“°ëŠ” RNN ë ˆì´ì–´
from dev.layers.activation_layer import Activation
from dev.layers.dense import Dense
import cupy as cp
import graph_executor as ge  # ê·¸ë˜í”„ í™•ì¸/ì—­ì „íŒŒ í˜¸ì¶œìš©


def make_seq_parity_dataset(B=128, T=12, D=4, seed=0, threshold=None):
    """
    ì‹œí€€ìŠ¤ í•©(ì²« ë²ˆì§¸ í”¼ì²˜ ê¸°ì¤€)ì´ thresholdë³´ë‹¤ í¬ë©´ 1, ì•„ë‹ˆë©´ 0.
    X: (B, T, D) ~ U(0,1)
    y: (B, 1) in {0,1}
    """
    rng = np.random.default_rng(seed)
    x = rng.random((B, T, D), dtype=np.float32)
    # ì²« í”¼ì²˜ì˜ í•©ì„ ê¸°ì¤€ìœ¼ë¡œ ë¼ë²¨ ìƒì„±
    sums = x[:, :, 0].sum(axis=1)
    if threshold is None:
        threshold = np.median(sums)  # ì¤‘ê°„ê°’ ê¸°ì¤€
    y = (sums > threshold).astype(np.float32).reshape(B, 1)
    return x.astype(np.float32), y.astype(np.float32)


def dump_grad_ptr_keys(model, xb, yb, tag):
    """run_graph_backward_entryë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ grads_ptrsì˜ í‚¤ì™€ shapeë¥¼ ì¶œë ¥."""
    xb_cp = cp.asarray(xb, dtype=cp.float32)
    yb_cp = cp.asarray(yb, dtype=cp.float32)

    # y_true shape ë“±ë¡(í•„ìš” ì‹œ 1íšŒë§Œ)
    model._ensure_label_shape(yb_cp)

    # í…ì„œ í¬ì¸í„° ë°”ì¸ë”©
    tensor_ptrs = {"input": xb_cp.data.ptr, "y_true": yb_cp.data.ptr}
    for name, arr in model.weights.items():
        tensor_ptrs[name] = arr.data.ptr
    for name, arr in model.biases.items():
        tensor_ptrs[name] = arr.data.ptr

    grads_ptrs = ge.run_graph_backward_entry(
        E=model.E,
        tensors=tensor_ptrs,
        shapes=model.shapes,
        gradients={},                     # C++ì—ì„œ ì±„ì›Œ ë°˜í™˜
        final_output_id=model.output_var, # ì†ì‹¤ ì´ì „ ì¶œë ¥(=y_pred)
        batch_size=xb_cp.shape[0]
    )

    keys = sorted(grads_ptrs.keys())
    print(f"\n=== grads_ptrs keys ({tag}) ===")
    print("â†’", keys if keys else "âˆ… (none)")

    # ê° í‚¤ì˜ shapeë„ ê°™ì´ ì¶œë ¥
    for k in keys:
        shp = model.shapes.get(k, None)
        if shp is None:
            print(f"  - {k}: shape = (unknown)")
        else:
            print(f"  - {k}: shape = ({shp.rows},{shp.cols})")

    # ë ˆí¼ëŸ°ìŠ¤: í˜„ì¬ ëª¨ë¸ì´ ê°€ì§„ íŒŒë¼ë¯¸í„° í‚¤
    print("\nweights keys:", sorted(model.weights.keys()))
    print("biases  keys:", sorted(model.biases.keys()))
    return grads_ptrs


def print_graph(model):
    """Graph E ë…¸ë“œë“¤ì„ input/param/output ë° shapeê¹Œì§€ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    import graph_executor as ge

    print("\n=== [Graph E] ===")
    OPNAME = {
        ge.OpType.MATMUL:     "MATMUL",
        ge.OpType.ADD:        "ADD",
        ge.OpType.RELU:       "RELU",
        ge.OpType.SIGMOID:    "SIGMOID",
        ge.OpType.TANH:       "TANH",
        ge.OpType.FLATTEN:    "FLATTEN",
        ge.OpType.CONV2D:     "CONV2D",
        ge.OpType.LOSS:       "LOSS",
        ge.OpType.LEAKY_RELU: "LEAKY_RELU",
        ge.OpType.ELU:        "ELU",
        ge.OpType.GELU:       "GELU",
        ge.OpType.SILU:       "SILU",
        ge.OpType.SOFTMAX:    "SOFTMAX",
    }
    # ì„ íƒì ìœ¼ë¡œ RNN enumì´ ìˆì„ ë•Œë§Œ ì¶”ê°€
    if hasattr(ge.OpType, "RNN"):
        OPNAME[ge.OpType.RNN] = "RNN"
    if hasattr(ge.OpType, "POOL_MAX"):
        OPNAME[ge.OpType.POOL_MAX] = "POOL_MAX"
    if hasattr(ge.OpType, "POOL_AVG"):
        OPNAME[ge.OpType.POOL_AVG] = "POOL_AVG"

    def shape_str(shapes, tid):
        s = shapes.get(tid)
        return f"({s.rows},{s.cols})" if s else "(?)"

    for i, op in enumerate(model.E):
        tname = OPNAME.get(op.op_type, str(op.op_type))

        # ë²¡í„°/ë ˆê±°ì‹œ í˜¼ìš© ëŒ€ì‘
        inputs = getattr(op, "inputs", []) or ([op.input_id] if getattr(op, "input_id", "") else [])
        params = getattr(op, "params", []) or ([op.param_id] if getattr(op, "param_id", "") else [])

        in_str  = ", ".join(inputs) if inputs else "-"
        par_str = ", ".join(params) if params else "-"

        # ëŒ€í‘œ in/out/paramë¡œ shape í‘œì‹œ
        pin  = inputs[0] if inputs else ""
        pout = op.output_id
        sin  = shape_str(model.shapes, pin) if pin else "-"
        sout = shape_str(model.shapes, pout)

        # paramsëŠ” ëª¨ë‘ shape ë‚˜ì—´
        spar = ", ".join([f"{pid}:{shape_str(model.shapes, pid)}" for pid in params]) or "-"

        print(f"[{i}] {tname:<9} | in=[{in_str:<20}] {sin:<10} | params=[{spar}] | out={pout:<16} {sout}")

        # ---- extras pretty print ----
        ex = getattr(op, "extra_params", None)
        if ex is not None:
            if hasattr(ge.OpType, "CONV2D") and op.op_type == ge.OpType.CONV2D:
                print(f"     â†³ B={ex.batch_size} Cin={ex.input_c} HxW={ex.input_h}x{ex.input_w} "
                      f"â†’ Cout={ex.output_c} KhxKw={ex.kernel_h}x{ex.kernel_w} "
                      f"stride={ex.stride_h}x{ex.stride_w} pad={ex.padding_h}x{ex.padding_w}")
            if hasattr(ge.OpType, "POOL_MAX") and op.op_type in (getattr(ge.OpType, "POOL_MAX", -1),
                                                                 getattr(ge.OpType, "POOL_AVG", -1)):
                print(f"     â†³ B={ex.batch_size} C={ex.input_c} HxW={ex.input_h}x{ex.input_w} "
                      f"KhxKw={ex.kernel_h}x{ex.kernel_w} stride={ex.stride_h}x{ex.stride_w} "
                      f"pad={ex.padding_h}x{ex.padding_w} dilation={ex.dilation_h}x{ex.dilation_w} "
                      f"count_include_pad={ex.count_include_pad}")
            if hasattr(ge.OpType, "RNN") and op.op_type == ge.OpType.RNN:
                # ì£¼ì˜: activation ì½”ë“œë¥¼ extra.axisì— ì„ì‹œ ì €ì¥í–ˆì—ˆë‹¤ë©´ ê·¸ëŒ€ë¡œ í‘œì‹œ
                print(f"     â†³ B={ex.batch_size} T={ex.time_steps} D={ex.input_w} H={ex.hidden_size} "
                      f"act_code={ex.axis} use_bias={ex.use_bias}")
            if op.op_type == ge.OpType.LOSS:
                print(f"     â†³ loss_type='{ex.loss_type}' label_id='{ex.label_id}'")


def test_rnn_seq_parity():
    print("\n=== [TEST] RNN (single-op) â€” sequence parity BCE ===")

    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    B, T, D = 128, 12, 4
    H = 32  # hidden size

    # ë°ì´í„° ìƒì„±
    x, y = make_seq_parity_dataset(B=B, T=T, D=D, seed=123)

    # ëª¨ë¸ êµ¬ì„± â€” RNN(H) â†’ Dense(1) â†’ Sigmoid
    #   * RNN ë ˆì´ì–´ëŠ” ë‚´ë¶€ì—ì„œ ë‹¨ì¼ ì˜¤í¼(RNN)ë¡œ to_e_matrix êµ¬ì„±
    model = Sequential(input_shape=(B, T, D))
    model.add(RNN(units=H, activation=np.tanh, input_shape=(B, T, D), name="rnn",
                  use_backend_init=False))   # íŒŒë¼ë¯¸í„°ë¥¼ ì—”ì§„ìª½ìœ¼ë¡œ ìœ„ì„(ì„ íƒ)
    model.add(Dense(units=1, activation=None, initializer="xavier", name="fc"))
    model.add(Activation("sigmoid", name="sigm"))

    # ì»´íŒŒì¼ (BCE + SGD)
    model.compile(optimizer="sgd", loss="bce", learning_rate=0.05)

    # ê·¸ë˜í”„ ë…¸ë“œ ì¶œë ¥
    print_graph(model)

    # í•™ìŠµ ì „ ì†ì‹¤
    print("\n[BEFORE] evaluate on full set")
    loss_before = model.evaluate(x, y)
    print(f"  BCE(before): {loss_before:.6f}")

    # ğŸ” í•™ìŠµ ì „ grads_ptrs í‚¤ ì¶œë ¥
    dump_grad_ptr_keys(model, x[:16], y[:16], tag="before training")

    # í•™ìŠµ
    #  - RNNì€ ì‹œí€€ìŠ¤ í•™ìŠµ ë‚œì´ë„ê°€ ìˆìœ¼ë¯€ë¡œ ì—í­/ëŸ¬ë‹ë ˆì´íŠ¸ëŠ” ì ë‹¹íˆ ì¡°ì •
    model.fit(x, y, epochs=5000, batch_size=B, verbose=1)

    # í•™ìŠµ í›„ ì†ì‹¤
    print("\n[AFTER] evaluate on full set")
    loss_after = model.evaluate(x, y)
    print(f"  BCE(after):  {loss_after:.6f}")

    # ğŸ” í•™ìŠµ í›„ grads_ptrs í‚¤ ì¶œë ¥
    dump_grad_ptr_keys(model, x[:16], y[:16], tag="after training")

    # ì˜ˆì¸¡ ì¶œë ¥ (ì• 10ê°œ)
    y_pred = model.predict(x)
    print("\nğŸ” ì˜ˆì¸¡ ê²°ê³¼ (ì• 10ê°œ):")
    print("====================================")
    print("  ì •ë‹µ  |  ì˜ˆì¸¡ê°’(sigmoid)")
    print("--------|------------------")
    for i in range(min(10, B)):
        print(f"   {int(y[i,0])}   |   {float(y_pred[i,0]):.4f}")
    print("====================================")


if __name__ == "__main__":
    test_rnn_seq_parity()
